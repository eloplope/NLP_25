{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - NLP and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the weekly assignment of week 1. Note that the assignments are split up per lecture. \n",
    "Please upload your solutions on LearnIt as a notebook file to receive feedback.\n",
    "\n",
    "\n",
    "# Lecture 1. What are words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Regular Expressions\n",
    "For this section, it might be handy to use the website https://regex101.com/ to test your solutions.\n",
    "\n",
    "- a) Write a regular expression (regex or pattern) that matches any of the following words: `cat`, `sat`, `mat`.\n",
    "<br>\n",
    "(Bonus: What is a possible long solution? Can you find a shorter solution? *hint*: match characters instead of words)\n",
    "- b) Write a regular expression that matches numbers, e.g. `12`, `1,000`, `39.95`.\n",
    "- c) Expand the previous solution to match Danish price indications, e.g., `1,000 kr` or `39.95 DKK` or `19.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "**Note that we here use the old-fashioned meaning of tokenization which is the task of word segmentation (not to be confused with subword tokenization,which will be introduced in a later lecture**\n",
    "\n",
    "(Adapted notebook from S. Riedel, UCL & Facebook: https://github.com/uclnlp/stat-nlp-book).\n",
    "\n",
    "In Python, a simple way to tokenize a text is via the `split` method that divides a text wherever a particular substring is found. In the code below this pattern is simply the whitespace character, and this seems like a reasonable starting point for an English tokenization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Mr. Bob Dobolina is thinkin' of a master plan.\n",
    "Why doesn't he quit?\"\"\"\n",
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make more fine-grained decisions, we will focus on using regular expressions for tokenization in this assignment. This can be done by either:\n",
    "1. Defining the character sequence patterns at which to split.\n",
    "2. Specifying patters that define what constitutes a token. \n",
    "\n",
    "In the code below we use a simple pattern `\\s` that matches **any whitespace** to define where to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "gap = re.compile(r'\\s')\n",
    "gap.split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One **shortcoming** of this tokenization is its treatment of punctuation because it considers `plan.` as a token whereas ideally we would prefer `plan` and `.` to be distinct tokens. It might be easier to address this problem if we define what a token is, instead of what constitutes a gap. Below we have defined tokens as sequences of alphanumeric characters and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = re.compile(r'\\w+|[.?:]')\n",
    "token.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still isn't perfect as `Mr.` is split into two tokens, but it should be a single token. Moreover, we have actually lost an apostrophe. Both are fixed below, although we now fail to break up the contraction `doesn't`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = re.compile(r'Mr.|[\\w\\']+|[.?]')\n",
    "tokens = token.findall(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we have an input text and apply the tokenizer (described previously) on the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Curiouser\", 'and', 'curiouser', \"'\", 'cried', 'Alice', 'she', 'was', 'so', 'much']\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"'Curiouser and curiouser!' cried Alice (she was so much surprised, that for the moment she quite\n",
    "forgot how to speak good English); 'now I'm opening out like the largest telescope that ever was! Good-bye,\n",
    "feet!' (for when she looked down at her feet, they seemed to be almost out of sight, they were getting so far\n",
    "off). 'Oh, my poor little feet, I wonder who will put on your shoes and stockings for you now, dears? I'm sure I\n",
    "shan't be able! I shall be a great deal too far off to trouble myself about you: you must manage the best\n",
    "way you can; â€”but I must be kind to them,' thought Alice, 'or perhaps they won't walk the way I want to go!\n",
    "Let me see: I'll give them a new pair of boots every Christmas...'\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile(r'Mr.|[\\w\\']+|[.?]')\n",
    "tokens = token.findall(text)\n",
    "print(tokens[:10])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* a) The tokenizer clearly makes a few mistakes with punctuations. Where?\n",
    "\n",
    "* b) Write a tokenizer to separate all tokenization characters as a single token. Note that there is no \\p for punctuation in python `re` (so either make a list yourself, or you could specify it is any character which is not alphanumeric/whitespace).\n",
    "\n",
    "* c) Should one separate `'m`, `'ll`, `n't`, possessives, and other forms of contractions from the word? Implement a tokenizer that separates these, and attaches the `'` to the latter part of the contraction (so that I'm -> I 'm and can't -> can 't).\n",
    "\n",
    "* d) Should elipsis (...) be considered as three `.`s or one `...`? Design a regular expression that keeps the `.` together in 1 token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Twitter Tokenization\n",
    "As you might imagine, tokenizing tweets differs from standard tokenization. There are 'rules' on what specific elements of a tweet might be (mentions, hashtags, links), and how they are tokenized. The goal of this exercise is not to create a bullet-proof Twitter tokenizer but to understand tokenization in a different domain.\n",
    "\n",
    "In the next exercises, we will focus on the following tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"@robv New vids coming tomorrow #excited_as_a_child, can't w8!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = re.compile(r'[\\w]+')\n",
    "tokens = token.findall(tweet)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- a) What is the correct tokenization of the tweet above according to you?\n",
    "- b) Try your tokenizer from the previous exercise (Question 2). Which cases are going wrong? Rewrite your tokenizer such that it handles the above tweet correctly.\n",
    "- c) How will your tokenizer handle emojis?\n",
    "- d) Think of at least one example where your tokenizer (from b) will behave incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation is not a trivial task either.\n",
    "\n",
    "First, make sure you understand the following sentence segmentation code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_segment(match_regex, tokens):\n",
    "    \"\"\"\n",
    "    Splits a sequence of tokens into sentences, splitting wherever the given matching regular expression\n",
    "    matches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens      the input sequence as list of strings (each item is a ``word'')\n",
    "    match_regex the regular expression that defines at which token to split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a list of lists of strings, where each string is a word, and each inner list\n",
    "    represents a sentence.\n",
    "    \"\"\"\n",
    "    sentences = [[]]\n",
    "    for tok in tokens:\n",
    "        sentences[-1].append(tok)\n",
    "        if match_regex.match(tok):\n",
    "            sentences.append([])\n",
    "            \n",
    "    if sentences[-1] == []:\n",
    "        del sentences[-1]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, there is a variable `text` containing a small text and a regular expression-based segmenter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one', 'word', 'placename', 'in', 'U', '.']\n",
      "['K', '.']\n",
      "[\"Isn't\", 'that', 'weird', '?', 'I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?', 'Of', 'course', 'U', '.']\n",
      "['S', '.']\n",
      "['A', '.']\n",
      "['also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '...']\n",
      "['This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '.']\n",
      "[\"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http', 'www', '.']\n",
      "['wikipedia', '.']\n",
      "['org', 'during', 'their', 'Ph', '.']\n",
      "['D', '.']\n",
      "['or', 'an', 'M', '.']\n",
      "['Sc', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch is the longest official one-word placename in U.K. Isn't that weird? I mean, someone took the effort to really make this name as complicated as possible, huh?! Of course, U.S.A. also has its own record in the longest name, albeit a bit shorter... This record belongs to the place called Chargoggagoggmanchauggagoggchaubunagungamaugg. There's so many wonderful little details one can find out while browsing http://www.wikipedia.org during their Ph.D. or an M.Sc.\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile(r'Mr.|[\\w\\']+|[.?]+')\n",
    "\n",
    "tokens = token.findall(text)\n",
    "sentences = sentence_segment(re.compile(r'\\.'), tokens)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- a) Improve the segmenter so that it handles a list of abbreviations (U.S./U.K./Ph.D/M.Sc) correctly.\n",
    "- b) Improve the segmenter so that it handles URL's correctly. You can assume that URLs start with \"www.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization competition\n",
    "\n",
    "Tokenization of social media can be more challenging. We provide a small development set for you to experiment with, which you can find in `week1/tok.dev.txt`. The file is a tab-separated file, where the first column contains the input text, and the second column the gold tokenization (as decided by an annotator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line.strip().split('\\t') for line in open('tok.dev.txt', encoding=\"utf-8\")]\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a test file with the same format, but the gold annotation is missing. This can be found in `week1/tok.test.txt`. You are supposed to develop your tokenizer based on the development data, and then apply your tokenizer on the test data. You can hand in the predictions on the test data on LearnIt in the same slot as the rest of the assignment. We will use F1 score for evaluation.\n",
    "\n",
    "Make sure that the file you hand in:\n",
    "- Contains only the output of your system, so it should have the same number of lines as the test data file, but contains different placements of whitespace characters (the characters without whitespaces should be exactly the same though). \n",
    "- Has your ITU username as the name of the file: i.e. `robv.txt`.\n",
    "- You can test whether the format matches by using the evaluation script provided (described below) on the development data.\n",
    "\n",
    "We have provided an evaluation script for your convenience, it returns F1 score, recall, and precision. It also prints out all sentences where your model made an error (indicating the error in red if supported by your terminal), and checks whether your output is in the right format. It can be found in `week1/tok_eval.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Language (correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spelling correction\n",
    "\n",
    "Below is an implementation of the Levenshtein distance. It uses a some efficiency tricks, and it is not important you understand every line of this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshteinDistance('this', 'that'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide you with an English word list from [Aspell](http://aspell.net/) in `aspell-en-dict.txt`. It can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wordlist (one word per line)\n",
    "en_dict = set([word.strip() for word in open('aspell-en-dict.txt', encoding=\"utf-8\").readlines()])\n",
    "    \n",
    "# Example usage\n",
    "typo = 'brower'\n",
    "correction = 'browser'\n",
    "print(typo, correction)\n",
    "print(typo in en_dict)\n",
    "print(correction in en_dict)\n",
    "print(levenshteinDistance(typo, correction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Implement a (naive) spelling correction system that finds the word in the word list with the smallest minimum edit distance for a word that contains a misspelling. \n",
    "* b) There could be multiple words with the smallest minimum edit distance for some typos, what are supplementary methods to re-rank these? (mention at least 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation and Analysis of spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide you with a list of 100 typos and their corrections from the [GitHub Typo Corpus](https://aclanthology.org/2020.lrec-1.835/) in `typos.txt`. It can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load github typo corpus misspellings\n",
    "typos = []\n",
    "corrections = []\n",
    "for line in open('typos.txt', encoding=\"utf-8\"):\n",
    "    tok = line.strip().split('\\t')\n",
    "    typos.append(tok[0])\n",
    "    corrections.append(tok[1])\n",
    "    \n",
    "# Example usage\n",
    "print(typos[0], corrections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Evaluate the spelling correction system you implemented in the previous assignment with accuracy. How many of the words did it correct right?\n",
    "* b) Now evaluate the errors, can you identify some common causes (i.e. trends) in the mistakes of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Levenshtein distance in practice\n",
    "Work out the distance matrix of the edit distance for the two words: `dansk` and `dane`. This is not a programming assignment, but rather a pen-and-paper assignment. You can fill out a table in any editor you like, or draw the matrix on a piece of paper and upload a picture or scan. An example of such a matrix was shown in class, but is also included in the Speech and Language Processing book (figure 2.18)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
