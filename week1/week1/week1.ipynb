{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - NLP and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the weekly assignment of week 1. Note that the assignments are split up per lecture. \n",
    "Please upload your solutions on LearnIt as a notebook file to receive feedback.\n",
    "\n",
    "\n",
    "# Lecture 1. What are words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Regular Expressions\n",
    "For this section, it might be handy to use the website https://regex101.com/ to test your solutions.\n",
    "\n",
    "- a) Write a regular expression (regex or pattern) that matches any of the following words: `cat`, `sat`, `mat`.\n",
    "<br>\n",
    "(Bonus: What is a possible long solution? Can you find a shorter solution? *hint*: match characters instead of words)\n",
    "- b) Write a regular expression that matches numbers, e.g. `12`, `1,000`, `39.95`.\n",
    "- c) Expand the previous solution to match Danish price indications, e.g., `1,000 kr` or `39.95 DKK` or `19.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'sat', 'mat']\n",
      "['1.000', '39,95', '40']\n",
      "['100 kr', '39,95 DKK']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "##a\n",
    "text1=\"the cat is sat on the mat\"\n",
    "token1=re.compile('cat|sat|mat')\n",
    "print(token1.findall(text1))\n",
    "\n",
    "##b\n",
    "text2=\"1.000 bøfer blev 3 dage for gamle, og blev sat ned til 39,95 dkk, 40 menesker blev glade\"\n",
    "token2=re.compile('(\\d+[,.]?\\d+)')\n",
    "print(token2.findall(text2))\n",
    "\n",
    "##c\n",
    "\n",
    "text3=\"1.000 bøfer 12/4/4 blev 3 dage for gamle, og blev sat ned fra 100 kr til 39,95 DKK, 40 menesker blev glade\"\n",
    "token3=re.compile(r'\\d*[,.]?\\d*\\s?(?:kr|DKK)')\n",
    "print(token3.findall(text3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "**Note that we here use the old-fashioned meaning of tokenization which is the task of word segmentation (not to be confused with subword tokenization,which will be introduced in a later lecture**\n",
    "\n",
    "(Adapted notebook from S. Riedel, UCL & Facebook: https://github.com/uclnlp/stat-nlp-book).\n",
    "\n",
    "In Python, a simple way to tokenize a text is via the `split` method that divides a text wherever a particular substring is found. In the code below this pattern is simply the whitespace character, and this seems like a reasonable starting point for an English tokenization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan.\\nWhy',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit?']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Mr. Bob Dobolina is thinkin' of a master plan.\n",
    "Why doesn't he quit?\"\"\"\n",
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make more fine-grained decisions, we will focus on using regular expressions for tokenization in this assignment. This can be done by either:\n",
    "1. Defining the character sequence patterns at which to split.\n",
    "2. Specifying patters that define what constitutes a token. \n",
    "\n",
    "In the code below we use a simple pattern `\\s` that matches **any whitespace** to define where to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan.',\n",
       " 'Why',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit?']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "gap = re.compile(r'\\s')\n",
    "gap.split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One **shortcoming** of this tokenization is its treatment of punctuation because it considers `plan.` as a token whereas ideally we would prefer `plan` and `.` to be distinct tokens. It might be easier to address this problem if we define what a token is, instead of what constitutes a gap. Below we have defined tokens as sequences of alphanumeric characters and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr',\n",
       " '.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " 'thinkin',\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan',\n",
       " '.',\n",
       " 'Why',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'he',\n",
       " 'quit',\n",
       " '?']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = re.compile(r'\\w+|[.?:]')\n",
    "token.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still isn't perfect as `Mr.` is split into two tokens, but it should be a single token. Moreover, we have actually lost an apostrophe. Both are fixed below, although we now fail to break up the contraction `doesn't`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan',\n",
       " '.',\n",
       " 'Why',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit',\n",
       " '?']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = re.compile(r'Mr.|[\\w\\']+|[.?]')\n",
    "tokens = token.findall(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we have an input text and apply the tokenizer (described previously) on the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Curiouser\", 'and', 'curiouser', \"'\", 'cried', 'Alice', 'she', 'was', 'so', 'much']\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"'Curiouser and curiouser!' cried Alice (she was so much surprised, that for the moment she quite\n",
    "forgot how to speak good English); 'now I'm opening out like the largest telescope that ever was! Good-bye,\n",
    "feet!' (for when she looked down at her feet, they seemed to be almost out of sight, they were getting so far\n",
    "off). 'Oh, my poor little feet, I wonder who will put on your shoes and stockings for you now, dears? I'm sure I\n",
    "shan't be able! I shall be a great deal too far off to trouble myself about you: you must manage the best\n",
    "way you can; —but I must be kind to them,' thought Alice, 'or perhaps they won't walk the way I want to go!\n",
    "Let me see: I'll give them a new pair of boots every Christmas... . ...... .. .'\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile(r'Mr.|[\\w\\']+|[.?]')\n",
    "tokens = token.findall(text)\n",
    "print(tokens[:10])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* a) The tokenizer clearly makes a few mistakes with punctuations. Where?\n",
    "\n",
    "* b) Write a tokenizer to separate all tokenization characters as a single token. Note that there is no \\p for punctuation in python `re` (so either make a list yourself, or you could specify it is any character which is not alphanumeric/whitespace).\n",
    "\n",
    "* c) Should one separate `'m`, `'ll`, `n't`, possessives, and other forms of contractions from the word? Implement a tokenizer that separates these, and attaches the `'` to the latter part of the contraction (so that I'm -> I 'm and can't -> can 't).\n",
    "\n",
    "* d) Should elipsis (...) be considered as three `.`s or one `...`? Design a regular expression that keeps the `.` together in 1 token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers\n",
    "* a) \n",
    "1: the ' before the first word should be sepereate 2: we are losing the ! after curiouser!, and a ( is also lost\n",
    "\n",
    "\n",
    "* b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Curiouser', 'and', 'curiouser', '!', 'cried', 'Alice', '(', 'she', 'was', 'so']\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "token = re.compile(r'\\w+|[.?,!()]')\n",
    "tokens = token.findall(text)\n",
    "print(tokens[:10])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'pair', 'of', 'boots', 'every', 'Christmas', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', \"'\"]\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "token = re.compile(r'\\w+|\\'\\w*|[.?,!()\\']')\n",
    "tokens = token.findall(text)\n",
    "print(tokens[-20:])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* d)\n",
    "they should be keept together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', 'me', 'see', 'I', \"'ll\", 'give', 'them', 'a', 'new', 'pair', 'of', 'boots', 'every', 'Christmas', '...', '.', '......', '..', '.', \"'\"]\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "token = re.compile(r'\\w+|\\'\\w*|[?,!()\\']|\\.+')\n",
    "tokens = token.findall(text)\n",
    "print(tokens[-20:])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Twitter Tokenization\n",
    "As you might imagine, tokenizing tweets differs from standard tokenization. There are 'rules' on what specific elements of a tweet might be (mentions, hashtags, links), and how they are tokenized. The goal of this exercise is not to create a bullet-proof Twitter tokenizer but to understand tokenization in a different domain.\n",
    "\n",
    "In the next exercises, we will focus on the following tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"@robv New vids coming tomorrow #excited_as_a_child, can't w8!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['robv', 'New', 'vids', 'coming', 'tomorrow', 'excited_as_a_child', 'can', 't', 'w8']\n"
     ]
    }
   ],
   "source": [
    "token = re.compile(r'[\\w]+')\n",
    "tokens = token.findall(tweet)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- a) What is the correct tokenization of the tweet above according to you?\n",
    "- b) Try your tokenizer from the previous exercise (Question 2). Which cases are going wrong? Rewrite your tokenizer such that it handles the above tweet correctly.\n",
    "- c) How will your tokenizer handle emojis?\n",
    "- d) Think of at least one example where your tokenizer (from b) will behave incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers\n",
    "* a) \n",
    "1: \"@robv\", \"New\", \"vids\", \"coming\", \"tomorrow\", \"#excited_as_a_child\" ,\",\", ,\"can\" ,\"'t\", ,\"w8\",\"!!\"\"\n",
    "\n",
    "* b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['robv', 'New', 'vids', 'coming', 'tomorrow', 'excited_as_a_child', ',', 'can', \"'t\", 'w8', '!', '!']\n",
      "12\n",
      "['@robv', 'New', 'vids', 'coming', 'tomorrow', '#excited_as_a_child', ',', 'can', \"'t\", 'w8', '!', '!']\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "#og\n",
    "token = re.compile(r'\\w+|\\'\\w*|[?,!()\\']|\\.+')\n",
    "tokens = token.findall(tweet)\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "\n",
    "#new\n",
    "token = re.compile(r'(?:[@|\\#])?\\w+|\\'\\w*|[?,!()\\']|\\.+')\n",
    "tokens = token.findall(tweet)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* c) i assume it will skip emojies, since they arent included in the specialtegn list\n",
    "lets find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@robv', 'New', 'vids', 'coming', 'tomorrow', '#excited_as_a_child', ',', 'can', \"'t\", 'w8', '!', '!']\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "emojitxt=\"@robv New vids coming tomorrow #excited_as_a_child, can't w8!! 🍆 💦\"\n",
    "tokens = token.findall(emojitxt)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* d) as ilustrated it doesn't work for emojies correctly, and the mr. ,ms., etc. etc. will be split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation is not a trivial task either.\n",
    "\n",
    "First, make sure you understand the following sentence segmentation code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_segment(match_regex, tokens):\n",
    "    \"\"\"\n",
    "    Splits a sequence of tokens into sentences, splitting wherever the given matching regular expression\n",
    "    matches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens      the input sequence as list of strings (each item is a ``word'')\n",
    "    match_regex the regular expression that defines at which token to split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a list of lists of strings, where each string is a word, and each inner list\n",
    "    represents a sentence.\n",
    "    \"\"\"\n",
    "    sentences = [[]]\n",
    "    for tok in tokens:\n",
    "        sentences[-1].append(tok)\n",
    "        if match_regex.match(tok):\n",
    "            sentences.append([])\n",
    "            \n",
    "    if sentences[-1] == []:\n",
    "        del sentences[-1]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, there is a variable `text` containing a small text and a regular expression-based segmenter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one', 'word', 'placename', 'in', 'U', '.']\n",
      "['K', '.']\n",
      "[\"Isn't\", 'that', 'weird', '?', 'I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?', 'Of', 'course', 'U', '.']\n",
      "['S', '.']\n",
      "['A', '.']\n",
      "['also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '...']\n",
      "['This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '.']\n",
      "[\"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http', 'www', '.']\n",
      "['wikipedia', '.']\n",
      "['org', 'during', 'their', 'Ph', '.']\n",
      "['D', '.']\n",
      "['or', 'an', 'M', '.']\n",
      "['Sc', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch is the longest official one-word placename in U.K. Isn't that weird? I mean, someone took the effort to really make this name as complicated as possible, huh?! Of course, U.S.A. also has its own record in the longest name, albeit a bit shorter... This record belongs to the place called Chargoggagoggmanchauggagoggchaubunagungamaugg. There's so many wonderful little details one can find out while browsing http://www.wikipedia.org during their Ph.D. or an M.Sc.\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile(r'Mr.|[\\w\\']+|[.?]+')\n",
    "\n",
    "tokens = token.findall(text)\n",
    "sentences = sentence_segment(re.compile(r'\\.'), tokens)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- a) Improve the segmenter so that it handles a list of abbreviations (U.S./U.K./Ph.D/M.Sc) correctly.\n",
    "- b) Improve the segmenter so that it handles URL's correctly. You can assume that URLs start with \"www.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers:\n",
    "\n",
    "* a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_segment(match_regex, tokens):\n",
    "    \"\"\"\n",
    "    Splits a sequence of tokens into sentences, splitting wherever the given matching regular expression\n",
    "    matches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens      the input sequence as list of strings (each item is a ``word'')\n",
    "    match_regex the regular expression that defines at which token to split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a list of lists of strings, where each string is a word, and each inner list\n",
    "    represents a sentence.\n",
    "    \"\"\"\n",
    "    abbreviations={'U','K','S','A', 'Ph', 'M', 'Dr', 'Mr', 'Mrs', 'Inc', 'Ltd', 'Co'}\n",
    "    sentences = [[]]\n",
    "    for i, tok in enumerate(tokens):\n",
    "        sentences[-1].append(tok)\n",
    "        if match_regex.match(tok) and tokens[i-1] not in abbreviations:\n",
    "            sentences.append([])\n",
    "            \n",
    "    if sentences[-1] == []:\n",
    "        del sentences[-1]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one', 'word', 'placename', 'in', 'U', '.', 'K', '.', \"Isn't\", 'that', 'weird', '?']\n",
      "['I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?']\n",
      "['Of', 'course', 'U', '.', 'S', '.', 'A', '.', 'also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '...']\n",
      "['This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '.']\n",
      "[\"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http', 'www', '.']\n",
      "['wikipedia', '.']\n",
      "['org', 'during', 'their', 'Ph', '.', 'D', '.']\n",
      "['or', 'an', 'M', '.', 'Sc', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch is the longest official one-word placename in U.K. Isn't that weird? I mean, someone took the effort to really make this name as complicated as possible, huh?! Of course, U.S.A. also has its own record in the longest name, albeit a bit shorter... This record belongs to the place called Chargoggagoggmanchauggagoggchaubunagungamaugg. There's so many wonderful little details one can find out while browsing http://www.wikipedia.org during their Ph.D. or an M.Sc.\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile(r'Mr.|[\\w\\']+|[.?]+')\n",
    "\n",
    "tokens = token.findall(text)\n",
    "sentences = sentence_segment(re.compile(r'[\\.?!]'), tokens)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_segment_link_state(match_regex, tokens):\n",
    "    linkindicator=re.compile(r'www|WWW|http|HTTP')\n",
    "    linkoffdicator=re.compile(r'com|org|dk|gov|io')\n",
    "    abbreviations={'U','K','S','A', 'Ph', 'M', 'Dr','D', 'Mr', 'Mrs', 'Inc', 'Ltd', 'Co'}\n",
    "    link=False\n",
    "    sentences = [[]]\n",
    "    for i, tok in enumerate(tokens):\n",
    "        sentences[-1].append(tok)\n",
    "        if linkindicator.match(tok) or link==True:\n",
    "            link=True\n",
    "            if  linkoffdicator.match(tok):                \n",
    "                link=False      \n",
    "        elif match_regex.match(tok) and tokens[i-1] not in abbreviations:\n",
    "            sentences.append([])\n",
    "            \n",
    "    if sentences[-1] == []:\n",
    "        del sentences[-1]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_segment_linkcheck(match_regex, tokens):\n",
    "    linkindicator=re.compile(r'www|WWW|http|HTTP')\n",
    "    linkoffdicator=re.compile(r'com|org|dk|gov|io')\n",
    "    abbreviations={'U','K','S','A', 'Ph', 'M', 'Dr','D', 'Mr', 'Mrs', 'Inc', 'Ltd', 'Co'}\n",
    "    sentences = [[]]\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        prev_token = tokens[i-1] if i > 0 else \"\"\n",
    "        next_token = tokens[i+1] if i < len(tokens)-1 else \"\"\n",
    "        sentences[-1].append(tok)\n",
    "        if match_regex.match(tok) and prev_token not in abbreviations and not linkindicator.match(prev_token) and not linkoffdicator.match(next_token):\n",
    "            sentences.append([])           \n",
    "    if sentences[-1] == []:\n",
    "        del sentences[-1]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one', 'word', 'placename', 'in', 'U', '.', 'K', '.', \"Isn't\", 'that', 'weird', '?']\n",
      "['I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?']\n",
      "['Of', 'course', 'U', '.', 'S', '.', 'A', '.', 'also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '...']\n",
      "['This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '.']\n",
      "[\"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http', 'www', '.', 'wikipedia', '.', 'org', 'during', 'their', 'Ph', '.', 'D', '.', 'or', 'an', 'M', '.', 'Sc', '.']\n",
      "\n",
      "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one', 'word', 'placename', 'in', 'U', '.', 'K', '.', \"Isn't\", 'that', 'weird', '?']\n",
      "['I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?']\n",
      "['Of', 'course', 'U', '.', 'S', '.', 'A', '.', 'also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '...']\n",
      "['This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '.']\n",
      "[\"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http', 'www', '.', 'wikipedia', '.', 'org', 'during', 'their', 'Ph', '.', 'D', '.', 'or', 'an', 'M', '.', 'Sc', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences1 = sentence_segment_link_state(re.compile(r'[\\.?!]'), tokens)\n",
    "sentences2 = sentence_segment_linkcheck(re.compile(r'[\\.?!]'), tokens)\n",
    "\n",
    "for sentence in sentences1:\n",
    "    print(sentence)\n",
    "print()\n",
    "\n",
    "for sentence in sentences2:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization competition\n",
    "\n",
    "Tokenization of social media can be more challenging. We provide a small development set for you to experiment with, which you can find in `week1/tok.dev.txt`. The file is a tab-separated file, where the first column contains the input text, and the second column the gold tokenization (as decided by an annotator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Found the scarriest mystery door in my school. I'M SO CURIOUS D:\", \"Found the scarriest mystery door in my school . I 'M SO CURIOUS D:\"]\n"
     ]
    }
   ],
   "source": [
    "data = [line.strip().split('\\t') for line in open('tok.dev.txt', encoding=\"utf-8\")]\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Found', 'the', 'scarriest', 'mystery', 'door', 'in', 'my', 'school', '.', 'I', \"'M\", 'SO', 'CURIOUS', 'D:']\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "old_school_emojis = [\n",
    "    r':\\)', r':D', r'D:', r':P', r':\\(', r';\\)', r':\\|', r':\\^D', r':O', r':/', r'<3',\n",
    "    r':3', r':\\*', r':-D', r':-\\)', r':-\\(', r':-P', r';-\\)', r';-\\(', r':-/', \n",
    "    r':<', r':>', r'B\\)', r';P', r'=\\)', r'X\\)', r'XD', r':c', r'=\\('\n",
    "]\n",
    "\n",
    "tester = \"Found the scarriest mystery door in my school. I'M SO CURIOUS D:\"\n",
    "\n",
    "# Create a regex pattern that includes old-school emojis\n",
    "emoji_pattern = '|'.join(old_school_emojis)\n",
    "\n",
    "# Define the regex pattern for words and other tokens\n",
    "token_pattern = r'(?:[@#])?\\w+|\\'\\w*|[?,!()\\']|\\.+|[^a-zA-Z\\s]'\n",
    "\n",
    "# Final pattern: Combine emoji matching first, followed by other tokens\n",
    "final_pattern = f'({emoji_pattern}|{token_pattern})'\n",
    "\n",
    "# Apply the regex pattern to the string\n",
    "tokens = re.findall(final_pattern, tester)\n",
    "print(tokens)\n",
    "print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a test file with the same format, but the gold annotation is missing. This can be found in `week1/tok.test.txt`. You are supposed to develop your tokenizer based on the development data, and then apply your tokenizer on the test data. You can hand in the predictions on the test data on LearnIt in the same slot as the rest of the assignment. We will use F1 score for evaluation.\n",
    "\n",
    "Make sure that the file you hand in:\n",
    "- Contains only the output of your system, so it should have the same number of lines as the test data file, but contains different placements of whitespace characters (the characters without whitespaces should be exactly the same though). \n",
    "- Has your ITU username as the name of the file: i.e. `robv.txt`.\n",
    "- You can test whether the format matches by using the evaluation script provided (described below) on the development data.\n",
    "\n",
    "We have provided an evaluation script for your convenience, it returns F1 score, recall, and precision. It also prints out all sentences where your model made an error (indicating the error in red if supported by your terminal), and checks whether your output is in the right format. It can be found in `week1/tok_eval.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Language (correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spelling correction\n",
    "\n",
    "Below is an implementation of the Levenshtein distance. It uses a some efficiency tricks, and it is not important you understand every line of this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#added swap for better results\n",
    "\n",
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshteinDistance('this', 'that'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = [[i + j if i * j == 0 else 0 for j in range(len(s2) + 1)] for i in range(len(s1) + 1)]\n",
    "    \n",
    "    for i1 in range(1, len(s1) + 1):\n",
    "        for i2 in range(1, len(s2) + 1):\n",
    "            if s1[i1 - 1] == s2[i2 - 1]:\n",
    "                distances[i1][i2] = distances[i1 - 1][i2 - 1]\n",
    "            else:\n",
    "                distances[i1][i2] = 1 + min(\n",
    "                    distances[i1 - 1][i2],     # Deletion\n",
    "                    distances[i1][i2 - 1],     # Insertion\n",
    "                    distances[i1 - 1][i2 - 1]  # Substitution\n",
    "                )\n",
    "                \n",
    "                # Check for transposition (swapping adjacent letters)\n",
    "                if i1 > 1 and i2 > 1 and s1[i1 - 1] == s2[i2 - 2] and s1[i1 - 2] == s2[i2 - 1]:\n",
    "                    distances[i1][i2] = min(distances[i1][i2], distances[i1 - 2][i2 - 2] + 1)\n",
    "    \n",
    "    return distances[-1][-1]\n",
    "\n",
    "print(levenshteinDistance('this', 'thsi'))  # Example usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide you with an English word list from [Aspell](http://aspell.net/) in `aspell-en-dict.txt`. It can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brower browser\n",
      "False\n",
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Load wordlist (one word per line)\n",
    "en_dict = set([word.strip() for word in open('aspell-en-dict.txt', encoding=\"utf-8\").readlines()])\n",
    "    \n",
    "# Example usage\n",
    "typo = 'brower'\n",
    "correction = 'browser'\n",
    "print(typo, correction)\n",
    "print(typo in en_dict)\n",
    "print(correction in en_dict)\n",
    "print(levenshteinDistance(typo, correction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Implement a (naive) spelling correction system that finds the word in the word list with the smallest minimum edit distance for a word that contains a misspelling. \n",
    "* b) There could be multiple words with the smallest minimum edit distance for some typos, what are supplementary methods to re-rank these? (mention at least 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'handle']]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def spellcheck(testword):\n",
    "    min_edit=10\n",
    "    min_edits=[]\n",
    "    for word in en_dict:\n",
    "        if (len(word) <= len(testword)+1) and (len(word) >= len(testword)-1):\n",
    "            dist=levenshteinDistance(testword, word)\n",
    "            if dist == min_edit:\n",
    "                min_edits.append([dist,word])\n",
    "            elif dist < min_edit:\n",
    "                min_edits=[[dist,word]]\n",
    "                min_edit=dist\n",
    "    return min_edits\n",
    "    \n",
    "spellcheck(\"hanlde\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation and Analysis of spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide you with a list of 100 typos and their corrections from the [GitHub Typo Corpus](https://aclanthology.org/2020.lrec-1.835/) in `typos.txt`. It can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "browers browser\n"
     ]
    }
   ],
   "source": [
    "# Load github typo corpus misspellings\n",
    "typos = []\n",
    "corrections = []\n",
    "for line in open('typos.txt', encoding=\"utf-8\"):\n",
    "    tok = line.strip().split('\\t')\n",
    "    typos.append(tok[0])\n",
    "    corrections.append(tok[1])\n",
    "    \n",
    "# Example usage\n",
    "print(typos[0], corrections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Evaluate the spelling correction system you implemented in the previous assignment with accuracy. How many of the words did it correct right?\n",
    "* b) Now evaluate the errors, can you identify some common causes (i.e. trends) in the mistakes of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  / 100 : browers -> brokers\n",
      "1  / 100 : asigned -> aligned\n",
      "2  / 100 : hanlde -> handle\n",
      "3  / 100 : poining -> pointing\n",
      "4  / 100 : pittsburg -> Pittsburgh\n",
      "5  / 100 : inequlity -> inequity\n",
      "6  / 100 : exeption -> exception\n",
      "7  / 100 : soem -> some\n",
      "8  / 100 : meagniful -> meaningful\n",
      "9  / 100 : securiy -> security\n",
      "10  / 100 : meassure -> reassure\n",
      "11  / 100 : dessa -> Odessa\n",
      "12  / 100 : buiild -> build\n",
      "13  / 100 : aproach -> approach\n",
      "14  / 100 : oldeer -> older\n",
      "15  / 100 : aroung -> around\n",
      "16  / 100 : repsond -> respond\n",
      "17  / 100 : explicliy -> explicit\n",
      "18  / 100 : tranlate -> translate\n",
      "19  / 100 : khói -> Thai\n",
      "20  / 100 : embedeed -> embedded\n",
      "21  / 100 : wriitten -> written\n",
      "22  / 100 : defailt -> default\n",
      "23  / 100 : kenrel -> kennel\n",
      "24  / 100 : adventage -> advantage\n",
      "25  / 100 : validater -> validate\n",
      "26  / 100 : initilise -> initialise\n",
      "27  / 100 : conise -> ionise\n",
      "28  / 100 : stephan -> Stephan\n",
      "29  / 100 : persitant -> persistent\n",
      "30  / 100 : evalution -> evaluation\n",
      "31  / 100 : rysnc -> sync\n",
      "32  / 100 : githuhb -> GitHub\n",
      "33  / 100 : locgical -> logical\n",
      "34  / 100 : rhe -> rhea\n",
      "35  / 100 : grenlets -> runlets\n",
      "36  / 100 : fultiple -> multiple\n",
      "37  / 100 : broswers -> browsers\n",
      "38  / 100 : succssful -> successful\n",
      "39  / 100 : legen -> legmen\n",
      "40  / 100 : unknwon -> unknown\n",
      "41  / 100 : alson -> also\n",
      "42  / 100 : availible -> available\n",
      "43  / 100 : fromm -> from\n",
      "44  / 100 : raiils -> rails\n",
      "45  / 100 : namespece -> namesake\n",
      "46  / 100 : hesistate -> hesitate\n",
      "47  / 100 : packege -> package\n",
      "48  / 100 : associted -> associated\n",
      "49  / 100 : provion -> provide\n",
      "50  / 100 : listenning -> listening\n",
      "51  / 100 : hieght -> height\n",
      "52  / 100 : developped -> developed\n",
      "53  / 100 : chosed -> hosed\n",
      "54  / 100 : sepcify -> specify\n",
      "55  / 100 : pareuet -> parquet\n",
      "56  / 100 : maxiumum -> maximum\n",
      "57  / 100 : uma -> Yuma\n",
      "58  / 100 : gutengerb -> Gutenberg\n",
      "59  / 100 : scss -> suss\n",
      "60  / 100 : candidated -> candidate\n",
      "61  / 100 : silumar -> lumbar\n",
      "62  / 100 : cunn -> Dunn\n",
      "63  / 100 : sofware -> software\n",
      "64  / 100 : syndrom -> syndrome\n",
      "65  / 100 : leidal -> medal\n",
      "66  / 100 : visusal -> visual\n",
      "67  / 100 : pylplt -> pulpit\n",
      "68  / 100 : appl -> appal\n",
      "69  / 100 : versiom -> version\n",
      "70  / 100 : folllowing -> following\n",
      "71  / 100 : txsockxs -> tussocks\n",
      "72  / 100 : apllies -> allies\n",
      "73  / 100 : lin -> in\n",
      "74  / 100 : opperating -> operating\n",
      "75  / 100 : dilligence -> diligence\n",
      "76  / 100 : joine -> joined\n",
      "77  / 100 : dien -> diet\n",
      "78  / 100 : cahce -> cache\n",
      "79  / 100 : cann -> cane\n",
      "80  / 100 : redirec -> redirect\n",
      "81  / 100 : standerd -> standers\n",
      "82  / 100 : numbrer -> number\n",
      "83  / 100 : guthub -> gutful\n",
      "84  / 100 : repnz -> reins\n",
      "85  / 100 : incease -> increase\n",
      "86  / 100 : repayed -> replayed\n",
      "87  / 100 : componets -> components\n",
      "88  / 100 : referennce -> reference\n",
      "89  / 100 : masskew -> passkey\n",
      "90  / 100 : divder -> divider\n",
      "91  / 100 : listern -> listen\n",
      "92  / 100 : keword -> reword\n",
      "93  / 100 : exeucting -> executing\n",
      "94  / 100 : hallowwing -> hallowing\n",
      "95  / 100 : hoook -> hook\n",
      "96  / 100 : rin -> in\n",
      "97  / 100 : impoort -> import\n",
      "98  / 100 : depened -> depend\n",
      "99  / 100 : goood -> good\n"
     ]
    }
   ],
   "source": [
    "mycorrections=[]\n",
    "for i, typo in enumerate(typos):\n",
    "    corrected=spellcheck(typo)[0][1]\n",
    "    mycorrections.append(corrected)\n",
    "    print(i,\" / 100 :\",typo, \"->\",corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycorrections = [s.lower() for s in mycorrections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6100\n",
      "Recall: 0.6100\n",
      "F1 Score: 0.7578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "# Example true labels and predictions\n",
    "y_true = corrections # Ground truth\n",
    "y_pred = mycorrections  # Predicted labels\n",
    "\n",
    "# Convert to binary: True (1) if same, False (0) if different\n",
    "y_binary_true = [1] * len(y_true)  # Always 1 (ground truth is always \"correct\")\n",
    "y_binary_pred = [1 if a == b else 0 for a, b in zip(y_true, y_pred)]\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_binary_true, y_binary_pred)\n",
    "recall = recall_score(y_binary_true, y_binary_pred, average=\"binary\")\n",
    "f1 = f1_score(y_binary_true, y_binary_pred, average=\"binary\")\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "browers browser brokers\n",
      "asigned assigned aligned\n",
      "hanlde handle handle\n",
      "poining pointing pointing\n",
      "pittsburg pittsburgh pittsburgh\n",
      "inequlity inequality inequity\n",
      "exeption exception exception\n",
      "soem some some\n",
      "meagniful meaningful meaningful\n",
      "securiy security security\n",
      "meassure measure reassure\n",
      "dessa essa odessa\n",
      "buiild build build\n",
      "aproach approach approach\n",
      "oldeer older older\n",
      "aroung around around\n",
      "repsond respond respond\n",
      "explicliy explicitly explicit\n",
      "tranlate translate translate\n",
      "khói khỏi thai\n",
      "embedeed embedded embedded\n",
      "wriitten written written\n",
      "defailt default default\n",
      "kenrel kernel kennel\n",
      "adventage advantage advantage\n",
      "validater validator validate\n",
      "initilise initialize initialise\n",
      "conise concise ionise\n",
      "stephan stephen stephan\n",
      "persitant persistent persistent\n",
      "evalution evaluation evaluation\n",
      "rysnc rsync sync\n",
      "githuhb github github\n",
      "locgical logical logical\n",
      "rhe the rhea\n",
      "grenlets greenlets runlets\n",
      "fultiple multiple multiple\n",
      "broswers browsers browsers\n",
      "succssful successful successful\n",
      "legen legend legmen\n",
      "unknwon unknown unknown\n",
      "alson also also\n",
      "availible available available\n",
      "fromm from from\n",
      "raiils rails rails\n",
      "namespece namespace namesake\n",
      "hesistate hesitate hesitate\n",
      "packege package package\n",
      "associted associated associated\n",
      "provion provision provide\n",
      "listenning listening listening\n",
      "hieght height height\n",
      "developped developed developed\n",
      "chosed chosen hosed\n",
      "sepcify specify specify\n",
      "pareuet parquet parquet\n",
      "maxiumum maximum maximum\n",
      "uma um yuma\n",
      "gutengerb gutenberg gutenberg\n",
      "scss css suss\n",
      "candidated candidate candidate\n",
      "silumar simular lumbar\n",
      "cunn cudnn dunn\n",
      "sofware software software\n",
      "syndrom syndrome syndrome\n",
      "leidal leidel medal\n",
      "visusal visual visual\n",
      "pylplt pyplot pulpit\n",
      "appl app appal\n",
      "versiom version version\n",
      "folllowing following following\n",
      "txsockxs txsocks tussocks\n",
      "apllies applies allies\n",
      "lin line in\n",
      "opperating operating operating\n",
      "dilligence diligence diligence\n",
      "joine joined joined\n",
      "dien dein diet\n",
      "cahce cache cache\n",
      "cann can cane\n",
      "redirec redirect redirect\n",
      "standerd standard standers\n",
      "numbrer number number\n",
      "guthub github gutful\n",
      "repnz repe reins\n",
      "incease increase increase\n",
      "repayed repaid replayed\n",
      "componets components components\n",
      "referennce reference reference\n",
      "masskew maxskew passkey\n",
      "divder divider divider\n",
      "listern listener listen\n",
      "keword keyword reword\n",
      "exeucting executing executing\n",
      "hallowwing hallowing hallowing\n",
      "hoook hook hook\n",
      "rin run in\n",
      "impoort import import\n",
      "depened depend depend\n",
      "goood good good\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(typos[i],y_true[i],y_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Levenshtein distance in practice\n",
    "Work out the distance matrix of the edit distance for the two words: `dansk` and `dane`. This is not a programming assignment, but rather a pen-and-paper assignment. You can fill out a table in any editor you like, or draw the matrix on a piece of paper and upload a picture or scan. An example of such a matrix was shown in class, but is also included in the Speech and Language Processing book (figure 2.18)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
