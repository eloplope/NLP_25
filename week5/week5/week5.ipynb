{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - NLP and Deep Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Sequence Prediction with HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement the Viterbi algorithm for decoding in sequence tagging. More concretely, we are going to build a POS tagger for English web data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Emissions and transition probabilities\n",
    "\n",
    "In this part of the exercise you are going to prepare the emission and transition probabilities to use in the viterbi algorithm. We are going to focus on the task of Parts-Of-Speech (POS) tagging. You can use the following datareader for the following assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(path):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[0])\n",
    "            current_tags.append(tok[1])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are going to use the POS labels as indices in our Viterbi matrix, we need to know all labels beforehand, and they need to have a static order. We also need a special beginning and end label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train data: 12543\n",
      "Length dev data: 2000\n",
      "All labels:\n",
      "['</S>', '<S>', 'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
      "Random datapoint:\n",
      "['It', 'is', 'a', 'time', 'to', 'learn', 'what', 'happened', 'and', 'how', 'it', 'may', 'affect', 'the', 'future', '.']\n",
      "['PRON', 'AUX', 'DET', 'NOUN', 'PART', 'VERB', 'PRON', 'VERB', 'CCONJ', 'ADV', 'PRON', 'AUX', 'VERB', 'DET', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "train_data = read_conll_file('pos-data/en_ewt-train.conll')\n",
    "dev_data = read_conll_file('pos-data/en_ewt-dev.conll')\n",
    "\n",
    "SMOOTH = 0.1\n",
    "BEG = '<S>'\n",
    "END = '</S>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "label_set = set([pos_label for sentence in train_data for pos_label in sentence[1]])\n",
    "label_set.add(BEG)\n",
    "label_set.add(END)\n",
    "# put labels in a list, so that they are guaranteed to have the same order\n",
    "label_list = list(sorted(label_set))\n",
    "\n",
    "print('Length train data: ' + str(len(train_data)))\n",
    "print('Length dev data: ' + str(len(dev_data)))\n",
    "\n",
    "print('All labels:')\n",
    "print(label_list)\n",
    "\n",
    "# the data is a list of pairs, containing 1: a list of words 2: a list of POS labels\n",
    "print('Random datapoint:')\n",
    "print(train_data[70][0])\n",
    "print(train_data[70][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) calculate the transition probabilities based on the training data, use a special label for the beginning of a sentence (`<S>`) and the end of a sentence (`</S>`). use laplace smoothing with a value of 0.1 to avoid probabilities of 0.0. In this case the UNK tag has a frequency of 0.1 to all all other tags (in most situations an UNK label is not to be expected to occur, but some POS tags can be rare).\n",
    "\n",
    "**Hint**: The transition probability $P(t_i|t_{i-1})$ is the probability that given a tag, $t_{i-1}$, that it will be followed by a tag $t_i$. \n",
    "$$P(t_i|t_{i-1}) = {C(t_{i-1},t_{i}) \\over C(t_{i-1})}$$\n",
    "With smoothing:\n",
    "$$P(t_i|t_{i-1}) = {C(t_{i-1},t_{i}) + \\gamma \\over C(t_{i-1}) + (|t|) * \\gamma} $$\n",
    "\n",
    "Where $(|t|-1) * \\gamma$ is used because we want add probability mass to all labels in the numerator (we have to match this mass in the denominator).\n",
    "\n",
    "**Hint2**: Every sentence in the data looks like `['DET', 'NOUN', 'VERB']` without any `<S>` or `</S>` tags. So the beginning and end of every data sample needs to be handled differently when counting occurences of transitions, alternatively you can add these tokens to each data sample so they look like `['<S>, 'DET', 'NOUN', 'VERB', </S>]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "6803.1\n",
      "0.5191660498019672\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary of dictionaries, so that we can easily query for a certain probability.\n",
    "# We add the smoothing value as a starting point, as it has to be added to each count.\n",
    "# Note that a list of lists with POS indices is more efficient (but a bit more cumbersome to implement).\n",
    "transition_counts = {label: {label: SMOOTH for label in label_list} for label in label_list}\n",
    "# The count that a NOUN follows an ADJ (empty now)\n",
    "print(transition_counts['ADJ']['NOUN'])\n",
    "\n",
    "# First obtain the raw counts\n",
    "import nltk\n",
    "for i in train_data:\n",
    "    bigrams = list(nltk.bigrams(i[1]))\n",
    "    for gram in bigrams:\n",
    "        transition_counts[gram[0]][gram[1]]+=1\n",
    "\n",
    "\n",
    "print(transition_counts['ADJ']['NOUN']) # should be 6803.1\n",
    "\n",
    "# Now fill the transition matrix, note that the outgoing probability of each label should sum to 1.0\n",
    "transition_probs = {label: {label: 0.0 for label in label_list} for label in label_list}\n",
    "# TODO\n",
    "for inlabel in label_list:\n",
    "    inlabelsum=sum(transition_counts[inlabel].values())\n",
    "    for outlabel in label_list:\n",
    "        transition_probs[inlabel][outlabel]=transition_counts[inlabel][outlabel]/inlabelsum\n",
    "\n",
    "        \n",
    "print(transition_probs['ADJ']['NOUN']) # 0.5191660498019673\n",
    "print(sum(transition_probs['ADJ'].values())) # 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) calculate the emission probabilities for all words in the training data. Make sure that every POS tag can be assigned to an `<UNK>` token, use laplace smoothing with a value of 0.1 to avoid probabilities of 0.0.\n",
    "\n",
    "**Hint**: The emission probability $P(w_i|t_{i})$ is the probability that given a tag, $t_i$, that it will be associated with a given word $w_i$. The formula below shows counts $C$ needed to calculate the probability.\n",
    "\n",
    "$$P(w_i|t_{i}) = {C(t_{i},w_{i}) \\over C(t_{i})}$$\n",
    "\n",
    "With Smoothing:\n",
    "\n",
    "$$P(w_i|t_{i}) = {C(t_{i},w_{i}) + \\gamma \\over C(t_{i}) + (|t|) * \\gamma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1\n",
      "0.1\n",
      "0.0004034631691038928\n",
      "6.614150313178571e-06\n"
     ]
    }
   ],
   "source": [
    "word_set = {UNK}\n",
    "for sent in train_data:\n",
    "    for word in sent[0]:\n",
    "        word_set.add(word)\n",
    "word_list = list(sorted(word_set))\n",
    "\n",
    "# Fill emission counts, start with smoothing value again\n",
    "emission_counts = {label: {word: SMOOTH for word in word_list} for label in label_list}\n",
    "# TODO\n",
    "for sentence in train_data:\n",
    "    for i in range(len(sentence[0])):\n",
    "        emission_counts[sentence[1][i]][sentence[0][i]]+=1\n",
    "\n",
    "print(emission_counts['ADJ']['European']) # 6.1\n",
    "print(emission_counts['ADJ']['Europe']) # 0.1\n",
    "\n",
    "# Convert to probabilities\n",
    "emission_probs = {label: {word: 0.0 for word in word_list} for label in label_list}\n",
    "# TODO o.O\n",
    "\n",
    "for inlabel in label_list:\n",
    "    inlabelsum=sum(emission_counts[inlabel].values())\n",
    "    for word in word_list:\n",
    "        emission_probs[inlabel][word]=emission_counts[inlabel][word]/inlabelsum\n",
    "      \n",
    "\n",
    "print(emission_probs['ADJ']['European']) # 0.00040346316910398104\n",
    "print(emission_probs['ADJ']['Europe']) # 6.614150313180017e-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check whether your solutions are correct by estimating the probabilities on the data and check whether the probabilities match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5191660498019672\n",
      "0.01141601160574333\n",
      "0.05263157894736841\n",
      "7.631315867795082e-06\n",
      "2.9909103515414666e-05\n",
      "0.0005740785225418476\n",
      "4.071478883275515e-06\n"
     ]
    }
   ],
   "source": [
    "print(transition_probs['ADJ']['NOUN']) # 0.5171926196793345\n",
    "print(transition_probs['NOUN']['ADJ']) # 0.01123434129302644\n",
    "print(transition_probs[BEG]['ADJ']) # 0.04082136964025221\n",
    "print(transition_probs['ADJ'][END]) # 0.003808756338424345\n",
    "print(emission_probs['NOUN']['calling'])   # 2.9909103515414666e-05\n",
    "print(emission_probs['VERB']['calling'])  # 0.0005740785225418476\n",
    "print(emission_probs['VERB'][UNK])  # 4.071478883275515e-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Viterbi algorithm\n",
    "\n",
    "In the image below we see an example of the calculation of the first 2 positions in a Viterbi decoding:\n",
    "<img width=500px src=\"pics/viterbi.jpg\">\n",
    "\n",
    "* a) Implement Viterbi decoding, use the transition and emission probabilities previously estimated (note that we also provide pre-calculated probabilities in `probs_en.pickle`). You can use the example code shown below as a starting point if you like.\n",
    "\n",
    "**Hint**: The implementation can become simpler if you think about the problem as a 2d matrix that needs to be filled (each position in the list is a node in the viterbi decoding, $v_1(7)$, $v_1(6)$, ...). You can first initialize the matrix with 0.0's, and then fill it from left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also load the pre-calculate probabilities:\n",
    "# import pickle\n",
    "# transition_probs, emission_probs = pickle.load(open('probs_en.pickle', 'rb'))\n",
    "import numpy as np\n",
    "def viterbi(sentence):\n",
    "    \"\"\"\n",
    "    sentence: list of strings\n",
    "    \"\"\"\n",
    "    columns = len(sentence)\n",
    "    # You don't need the special tokens in the viterbi decoding so we remove them\n",
    "    labels_list_exc_special = label_list.copy()\n",
    "    labels_list_exc_special.remove(BEG)\n",
    "    labels_list_exc_special.remove(END)\n",
    "\n",
    "    rows = len(labels_list_exc_special)\n",
    "    \n",
    "    # Create the full matrix for scores as well as the backtracking.\n",
    "    # e.g. scores[0][3] should get the probability of the best path of \n",
    "    # the first label and the 4th word in the sentence\n",
    "    # Backtrack contains the index of the best tag for the previous word\n",
    "    # e.g. backtrack[0][3] should get the index of the best tag for the 3rd word \n",
    "    # when backtracking from the first label and 4th word\n",
    "    scores = np.array([[0.0 for _ in range(columns)] for _ in range(rows)])\n",
    "    backtrack = np.array([[0 for _ in range(columns)] for _ in range(rows)])\n",
    "    \n",
    "    # Handle the first token separately, as it only has 2 probabilities (emission, transition)\n",
    "    word_position = 0\n",
    "    for pos_tag_idx, pos_tag in enumerate(labels_list_exc_special):\n",
    "        # The probability of the first word given the POS tag:\n",
    "        word = sentence[word_position]\n",
    "        if word not in emission_probs[pos_tag]:\n",
    "            word = UNK\n",
    "        em_prob = emission_probs[pos_tag][word] \n",
    "        \n",
    "        # The probability of the POS tag given that the previous \"tag\" is <S>\n",
    "        transition_prob = transition_probs[BEG][pos_tag]\n",
    "        \n",
    "        # Save the total probability:\n",
    "        scores[pos_tag_idx][word_position] = em_prob * transition_prob\n",
    "        \n",
    "        # Backtracking for the first token is uneccessary so we ignore it\n",
    "    \n",
    "    # Now handle the rest of the sequence\n",
    "    for word_position in range(1, columns):\n",
    "        for pos_tag_idx, pos_tag in enumerate(labels_list_exc_special):\n",
    "            word = sentence[word_position]\n",
    "            if word not in emission_probs[pos_tag]:\n",
    "                word = UNK\n",
    "\n",
    "            # Get emission probability\n",
    "            # TODO\n",
    "\n",
    "            # For each possible history path (i.e. label): get the total score\n",
    "            # We need to get the transition probability and the history probability\n",
    "            # Hint: the history probability is the score of the previous word position in scores matrix\n",
    "            # TODO\n",
    "            candidate_scores = [0.0] * len(labels_list_exc_special)\n",
    "            \n",
    "                \n",
    "            # Now extract the best score from candidate_scores and its previous path and save these\n",
    "            # Hint: backtrack should contain the index of the best tag:\n",
    "            # backtrack[tag_idx][word_position] = previous_best_tag_idx\n",
    "            # TODO\n",
    "            \n",
    "\n",
    "    # Extract the best score from the last labels to the special end label\n",
    "    # Hint: here you only have history and transition (no emission)\n",
    "    # TODO\n",
    "    \n",
    "    \n",
    "    # Extract the path from the best last label using the backtrack matrix\n",
    "    # Hint: the path contains the index of the best tag for each word\n",
    "    # TODO\n",
    "    current_path = [best_prev_idx]\n",
    "    \n",
    "    \n",
    "    # Reverse the path and convert the indexes to labels\n",
    "    # TODO\n",
    "    \n",
    "    \n",
    "    return \n",
    "\n",
    "viterbi(['this', 'is', 'a', 'very', 'good', 'chocolate', '.'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) Ensure that the best path is saved during the decoding, so that you can extract the labels. What is the accuracy (code for this below) of your implementation of the Viterbi algorithm on the development data (`pos-data/en_ewt-dev.conll`)?\n",
    "\n",
    "**Hint**: If implemented correctly, it should score at least an accuracy of 50%. If you score lower, we suggest you try printing the probabilities at each step (word) for the first sentence of the development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Loop through the development data and compute predictions\n",
    "for words, true_tags in dev_data:\n",
    "    predicted_tags = viterbi(words)  # Get the POS tags from your Viterbi algorithm\n",
    "    # Compare predicted tags with true tags\n",
    "    correct += sum(p == t for p, t in zip(predicted_tags, true_tags))\n",
    "    total += len(true_tags)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"Viterbi Accuracy on Development Data: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* c) **Bonus**: try to improve your predictions by inspecting common errors, tuning some of the decisions (e.g. smoothing, weighing the three probabilities) you made, or improving the handling of unknown tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Subword tokenization\n",
    "\n",
    "BERT models are trained to predict tokens that were masked with a special `[mask]` token. In this assignment you will inspect what it has learned, and whether it has certain preferences (i.e. probing). \n",
    "\n",
    "a) Load the multilingual Bert tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokzr = AutoTokenizer.from_pretrained('bert-base-multilingual-cased', use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilingual BERT was trained on the 100 most frequent languages of Wikipedia. They used smoothing, to correct inbalances in the data. However, their smoothing is relatively conservative, so high-resource languages have a higher impact on the model, and it is unclear how they sampled for training the tokenizer. Compare the tokenizations for two different language types you know; preferably one higher-resource and one lower-resource. If you only know 1 language, or only high-resource languages, try to use a different variety of the language (for example for English, use social media abbreviations or typos, e.g.: c u tmrw). Can you observe any differences in the results? does it match your intuition of separating mostly meaning-carrying subwords?\n",
    "\n",
    "You can use Figure 1 of https://arxiv.org/pdf/1911.02116.pdf or https://en.wikipedia.org/wiki/List_of_Wikipedias to see how large languages are on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Test whether the `bert-base-cased` model can solve the analogy task that we discussed in the word2vec lecture ([slides](https://github.itu.dk/robv/intro-nlp2023/blob/main/slides/07-vector_semantics.pdf), [assignment](https://github.itu.dk/robv/intro-nlp2023/blob/main/assignments/week4/week4.ipynb)), we can do this by masking the target word we are looking for, and let the model predict which words fit best. We can then use a prompt to discover what the language model would guess. For example, we can use the prompt \"man is to king as woman is to [MASK]\". Try at least two syntactic analogies, and two semantic analogies.\n",
    "You can use the following code:\n",
    "\n",
    "(Note that you need 4gb of RAM for this assignment, otherwise you can use the HPC.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM,AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def getTopN(inputSent, model, tokzr, topn=1):\n",
    "    maskId = tokzr.convert_tokens_to_ids(tokzr.mask_token)\n",
    "    tokenIds = tokzr(inputSent).input_ids\n",
    "    if maskId not in tokenIds:\n",
    "        return 'please include ' + tokzr.mask_token + ' in your input'\n",
    "    maskIndex = tokenIds.index(maskId)\n",
    "    logits = model(torch.tensor([tokenIds])).logits\n",
    "    return tokzr.convert_ids_to_tokens(torch.topk(logits, topn, dim=2).indices[0][maskIndex])\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained('bert-base-cased')\n",
    "tokzr = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "getTopN('This is a [MASK] test.', model, tokzr, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Test how robust the language model is, does it have an effect on the results of the word predictions if you include punctuations at the end of the sentence?, what about starting with a capital? and do typos have a large impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Think of some prompts that test whether the model has any gender biases, you can test this for example by using common gendered names or pronouns, swapping them and then check whether the predicted word changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Finetune a BERT model\n",
    "\n",
    "We have provided code for training a BERT based classifier, which can be found in `week5/bert/bert-topic.py`. The implementation uses huggingface's transformers library (https://github.com/huggingface/transformers), and simply adds a linear layer to convert the output of the CLS token from the last layer of the masked language model to a label. \n",
    "\n",
    "You can install the transformers library with the command:\n",
    "`pip3 install transformers`\n",
    "\n",
    "a) Inspect the code; what should the shape of the output_scores be at the end of the forward pass?, What does this output represent?\n",
    "\n",
    "b) Train the model on the HPC without a GPU, how long does it take? You can use the `scavenge` queue, and note that if you do not specify a GPU it will not use one. You can use a job script like:\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=bert-small\n",
    "#SBATCH --output=bert-small.out\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --time=10:00:00\n",
    "#SBATCH --mem=12G\n",
    "#SBATCH --mail-type=BEGIN,END,FAIL\n",
    "#SBATCH --partition=scavenge\n",
    "\n",
    "python3 # TODO\n",
    "```\n",
    "More information about the ITU HPC can be found on: https://hpc.itu.dk/ (only available on ITU network/VPN).\n",
    "\n",
    "c) Now change the number of maximum training sentences (MAX_TRAIN_SENTS) to 500 and the batch size (BATCH_SIZE) to 32. It will now take very long to train without a GPU. Train the model on the HPC, make sure you reserve a GPU to speed up the training, this can be done with `#SBATCH --gres=gpu`. Note that the code detects automatically whether a GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
