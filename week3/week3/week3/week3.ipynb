{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - NLP and Deep Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 - Language Identification with a Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, you will implement the forward step of a FFNN from scratch and compare your solution to Pytorch on a small toy example to predict the language for a given word. \n",
    "\n",
    "It is very important that you understand the basic building blocks (input/output: how to encode your instances, the labels; the model: what the neural network consists of, how to learn its weights, how to do a forward pass for prediction). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Representing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming multi-class classification tasks for the assignments of this week. The labels are: $$ y \\in \\{da,nl,en\\}$$\n",
    "\n",
    "We will use the same data as in week2, from:\n",
    "* English [Wookipedia](https://starwars.fandom.com/wiki/Main_Page)  \n",
    "* Danish [Kraftens Arkiver](https://starwars.fandom.com/da/wiki) \n",
    "* Dutch [Yodapedia](https://starwars.fandom.com/da/wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "def load_langid(path):\n",
    "    text = []\n",
    "    labels = []\n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "        tok = line.strip().split('\\t')\n",
    "        labels.append(tok[0])\n",
    "        text.append(tok[1])\n",
    "    return text, labels\n",
    "\n",
    "wooki_train_text, wooki_train_labels = load_langid('langid-data/wookipedia_langid.train.tok.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#wow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a): Convert the training data into n-hot format, where each feature represents whether a **single character** is present or not.  Similarly, convert the labels into numeric format. For simplicity, you can assume a closed vocabulary (only the letters in wookie_train_text, no unknown-character handling). Keep original casing, and assign the character indices based on their chronological order.\n",
    "\n",
    "  * What is the vocabulary size?\n",
    "  \n",
    "**Hint:** It is easier for the rest of the assignment if you directly use a torch tensor to save the features ([tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)), a 2d torch tensor filled with 0's can be initiated with: `torch.zeros(dim1, dim2, dtype=float)`. Note the use of `float` instead of `int` here, which is only because the `torch.mm` requires float tensors as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n",
      "['0', 'f', 'n', 'ï', ']', '|', 'ö', 'É', '°', 'ː', 'O', '–', 'θ', '“', 'u', 'k', '»', 'Å', ')', 'T', '4', '(', 'V', '[', 'à', '²', 'W', '1', 'e', '<', 'E', '.', ';', 'y', 'j', 'æ', '=', 'ə', 'g', '>', 'p', '8', '^', 'ō', 'm', '∑', 'a', '9', '«', 'B', 'è', 'ó', 'ɹ', 'q', '’', 'D', 'H', 'ü', 'd', 'J', ':', '6', '―', '…', '$', '#', 'Z', 'ś', 'b', '?', 'P', 'á', '™', 'Y', 'I', 'å', 'X', 'L', '´', 'A', 'o', \"'\", 'R', ',', 'Q', 'G', '&', 'K', '\\u200b', '5', 'N', 'i', 'ɑ', 't', 'w', 'ë', '½', '!', '%', '`', 'Ø', 'Æ', 'v', '7', '‘', 'ʊ', '-', 'C', 'é', 'l', 'x', 'Θ', ' ', '”', 'M', 'ń', '—', 's', '/', 'U', 'r', 'ø', '+', 'h', 'c', '2', 'F', 'z', 'S', '3', 'ñ']\n"
     ]
    }
   ],
   "source": [
    "tokens=[]\n",
    "for i in range(len(wooki_train_text)):\n",
    "    tokens += list(wooki_train_text[i])\n",
    "\n",
    "tokens=list(set(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOL_matrix=torch.zeros(len(wooki_train_text), len(tokens), dtype=torch.float)\n",
    "\n",
    "for s, sentence in enumerate(wooki_train_text):\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in sentence:\n",
    "            BOL_matrix[s][i]=1\n",
    "#BOL_list[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOL_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2: Forward pass (from scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Networks (FNNs) or MLPs\n",
    "\n",
    "Feedforward Neural Networks (FNNs) are also called Multilayer Perceptrons (MLPs). These are the most basic types of neural networks. They are called this way as the information is flowing from the input nodes through the network up to the output nodes. \n",
    "\n",
    "It is essential to understand that a neural network is a non-linear classification model which is based upon function application. Each layer in a neural network is an application of a function.\n",
    "\n",
    "Summary (by J.Frellsen):\n",
    "<img src=\"pics/fnn_jf.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to implement the forward step manually on a small dataset. You will create a network following the design in the following figure (note that the input should be the sames size as the number of characters found in the previous assignment, instead of 4):\n",
    "\n",
    "<img src=\"pics/nn.svg\">\n",
    "\n",
    "a) How many neurons do hidden layer 1 and hidden layer 2 have? Note: the bias node is not shown in the figure, you do not have to count them for this assignment.\n",
    "\n",
    "b) How many neurons does the output layer have? And the input layer? (Note: the figure shows only 4 input nodes, in this example your input size is defined in the previous assignment - what is the input layer size?)\n",
    "\n",
    "c) Specify the size of layers of the feedforward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions to determine the input and output dimensions of each layer\n",
    "input_dim = len(tokens)\n",
    "hidden_dim1 = 15\n",
    "hidden_dim2 = 20\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Now initialize the layers themselves as torch tensors (do not use a torch.module here!). You can define the bias and the weights in separate tensors. The weights should be initialized randomly (`torch.randn((dim1, dim2), dtype=torch.float)`, see also [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html)) and the biases can be set to 1 (`torch.ones(dim1, dtype=torch.float)`, see also [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html)). Confirm whether their size match the answer to `b)` and `a)` by printing .shape of the tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights1 shape: torch.Size([131, 15])\n",
      "Bias1 shape: torch.Size([15])\n",
      "Weights2 shape: torch.Size([15, 20])\n",
      "Bias2 shape: torch.Size([20])\n",
      "Weights3 shape: torch.Size([20, 3])\n",
      "Bias3 shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "## define all parameters of this NN\n",
    "\n",
    "# Initialize weights and biases for each layer\n",
    "# Layer 1: input -> hidden1\n",
    "weights1 = torch.randn((input_dim, hidden_dim1), dtype=torch.float)\n",
    "bias1 = torch.ones(hidden_dim1, dtype=torch.float)\n",
    "\n",
    "# Layer 2: hidden1 -> hidden2\n",
    "weights2 = torch.randn((hidden_dim1, hidden_dim2), dtype=torch.float)\n",
    "bias2 = torch.ones(hidden_dim2, dtype=torch.float)\n",
    "\n",
    "# Layer 3: hidden2 -> output\n",
    "weights3 = torch.randn((hidden_dim2, output_dim), dtype=torch.float)\n",
    "bias3 = torch.ones(output_dim, dtype=torch.float)\n",
    "\n",
    "# Print the shapes of the weights and biases to confirm dimensions\n",
    "print(\"Weights1 shape:\", weights1.shape)\n",
    "print(\"Bias1 shape:\", bias1.shape)\n",
    "\n",
    "print(\"Weights2 shape:\", weights2.shape)\n",
    "print(\"Bias2 shape:\", bias2.shape)\n",
    "\n",
    "print(\"Weights3 shape:\", weights3.shape)\n",
    "print(\"Bias3 shape:\", bias3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the shape of all parameters, we are ready to \"connect the dots\" and build the network. \n",
    "\n",
    "It is instructive to break the computation of each layer down into two steps: the scores $a1$ are obtained by the linear function followed by the activation applications $\\sigma$ to obtain the representation $z1$, as in:\n",
    "\n",
    "$$ a1 = xW_1 + b_1$$\n",
    "$$ z1 = \\sigma(a1)$$\n",
    "\n",
    "d) Specify the entire network up to the output layer $z3$, and **up to and exclusive** the final application of the softmax, the last activation function, which is provided. For multiplication [torch.mm](https://pytorch.org/docs/stable/generated/torch.mm.html) can be used. Use a tanh activation function: [torch.tanh](https://pytorch.org/docs/stable/generated/torch.tanh.html).\n",
    "\n",
    "The exact implementation of the softmax might differ from toolkit to toolkit (due to variations in implementation details in order to obtain numerical stability). Therefore, we will use the Pytorch implementation for the softmax calculation ([torch.nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output z3 (before softmax) for all samples in the training set: tensor([[-0.9990,  0.7394,  0.8762],\n",
      "        [-0.9823, -0.9970,  1.0000],\n",
      "        [-0.9954, -0.9959, -0.9569],\n",
      "        ...,\n",
      "        [-0.8334, -0.9985,  0.9997],\n",
      "        [-1.0000,  0.8612,  1.0000],\n",
      "        [-1.0000,  0.2778,  0.9845]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forward pass function to handle batch inputs\n",
    "def forward_pass(x):\n",
    "    # Layer 1: input -> hidden1\n",
    "    a1 = torch.mm(x, weights1) + bias1  # Linear function \n",
    "    z1 = torch.tanh(a1)  # Apply tanh activation\n",
    "    \n",
    "    # Layer 2: hidden1 -> hidden2\n",
    "    a2 = torch.mm(z1, weights2) + bias2\n",
    "    z2 = torch.tanh(a2)  # Apply tanh activation\n",
    "    \n",
    "    # Layer 3: hidden2 -> output\n",
    "    a3 = torch.mm(z2, weights3) + bias3\n",
    "    z3 = torch.tanh(a3)  # Apply tanh activation (before softmax)\n",
    "\n",
    "    return z3\n",
    "\n",
    "\n",
    "# Perform the forward pass on the entire training set\n",
    "output = forward_pass(BOL_matrix)\n",
    "\n",
    "print(\"Output z3 (before softmax) for all samples in the training set:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that all predictions sum up to approximately 1 (hint: use `torch.sum` with `axis=1`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6166, -0.9793, -2.9482,  ..., -0.8322,  0.8613,  0.2623])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(output,axis=1)\n",
    "#not quite one for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Congrats! you have made it through the manual construction of the forward pass. Note that these weights are still random, so performance is not expected to be good. Now lets compare your implementation to a set of pre-determined weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Where do the weights come from?  Loading existing weights\n",
    "\n",
    "So far, the model that you used randomly initialized weights. In this step we will load pre-trained model weights and do the forward pass with those weights, in order to check your implementation against model predictions computed by the toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to:\n",
    "* load pretrained weights for all parameters\n",
    "* apply the weights to the evaluation data\n",
    "* check that your manual softmax scores match the ones obtained by the pre-trained model `model` that we will load\n",
    "* convert the output to labels and calculate the accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load the pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elias\\AppData\\Local\\Temp\\ipykernel_8132\\1543763815.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lang_classifier = torch.load('model.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# use the character indexing from assignment 1\n",
    "idx2char = ['H', 'e', ' ', 'v', 'n', 'w', 't', 's', 'o', 'f', 'a', 'r', 'u', 'g', 'h', ',', 'i', 'c', 'y', 'd', 'b', 'm', 'p', 'l', 'k', '.', 'D', 'E', 'C', 'j', 'R', 'S', 'U', '1', \"'\", 'æ', 'å', 'q', '`', 'I', '(', ')', 'M', 'F', '-', 'x', 'K', '9', '5', 'B', 'W', 'z', 'G', 'P', 'L', '/', 'O', '6', 'T', '7', 'Z', '2', '0', 'J', 'V', 'A', 'ø', 'X', '–', 'N', 'ë', ':', '&', '3', 'Y', 'é', '4', '[', ']', '’', ';', '8', 'É', 'Æ', 'Q', '!', '—', 'ï', '°', 'ō', '\\u200b', '‘', 'ń', '“', '”', '?', 'Å', '<', '>', '#', '%', '+', 'ʊ', 'ɹ', 'ə', 'ɑ', 'ö', 'à', 'á', 'è', '=', 'ü', 'Ø', '∑', '^', 'ś', 'ñ', '|', '½', '$', '«', '™', 'ó', '´', '…', '―', '»', 'ː', 'θ', '²', 'Θ']\n",
    "char2idx = {'H': 0, 'e': 1, ' ': 2, 'v': 3, 'n': 4, 'w': 5, 't': 6, 's': 7, 'o': 8, 'f': 9, 'a': 10, 'r': 11, 'u': 12, 'g': 13, 'h': 14, ',': 15, 'i': 16, 'c': 17, 'y': 18, 'd': 19, 'b': 20, 'm': 21, 'p': 22, 'l': 23, 'k': 24, '.': 25, 'D': 26, 'E': 27, 'C': 28, 'j': 29, 'R': 30, 'S': 31, 'U': 32, '1': 33, \"'\": 34, 'æ': 35, 'å': 36, 'q': 37, '`': 38, 'I': 39, '(': 40, ')': 41, 'M': 42, 'F': 43, '-': 44, 'x': 45, 'K': 46, '9': 47, '5': 48, 'B': 49, 'W': 50, 'z': 51, 'G': 52, 'P': 53, 'L': 54, '/': 55, 'O': 56, '6': 57, 'T': 58, '7': 59, 'Z': 60, '2': 61, '0': 62, 'J': 63, 'V': 64, 'A': 65, 'ø': 66, 'X': 67, '–': 68, 'N': 69, 'ë': 70, ':': 71, '&': 72, '3': 73, 'Y': 74, 'é': 75, '4': 76, '[': 77, ']': 78, '’': 79, ';': 80, '8': 81, 'É': 82, 'Æ': 83, 'Q': 84, '!': 85, '—': 86, 'ï': 87, '°': 88, 'ō': 89, '\\u200b': 90, '‘': 91, 'ń': 92, '“': 93, '”': 94, '?': 95, 'Å': 96, '<': 97, '>': 98, '#': 99, '%': 100, '+': 101, 'ʊ': 102, 'ɹ': 103, 'ə': 104, 'ɑ': 105, 'ö': 106, 'à': 107, 'á': 108, 'è': 109, '=': 110, 'ü': 111, 'Ø': 112, '∑': 113, '^': 114, 'ś': 115, 'ñ': 116, '|': 117, '½': 118, '$': 119, '«': 120, '™': 121, 'ó': 122, '´': 123, '…': 124, '―': 125, '»': 126, 'ː': 127, 'θ': 128, '²': 129, 'Θ': 130}\n",
    "\n",
    "# the label indexes that were used during training\n",
    "label2idx = {'da':0, 'nl':1, 'en':2}\n",
    "idx2label = ['da', 'nl', 'en']\n",
    "\n",
    "# This is the definition of an FNN model in PyTorch, and can mostly be ignored for now.\n",
    "# We will focus on how to create Torch models in lecture 6\n",
    "class LangId(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(LangId, self).__init__()\n",
    "        self.input = nn.Linear(vocab_size, 15)\n",
    "        self.hidden1 = nn.Linear(15, 20)\n",
    "        self.hidden2 = nn.Linear(20, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.input(x))\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = self.hidden2(x)\n",
    "        return x\n",
    "\n",
    "lang_classifier = torch.load('model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the weights you just loaded using the `state_dict()` function of the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangId(\n",
      "  (input): Linear(in_features=131, out_features=15, bias=True)\n",
      "  (hidden1): Linear(in_features=15, out_features=20, bias=True)\n",
      "  (hidden2): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('input.weight',\n",
       "              tensor([[ 0.1274,  0.2723,  0.4691,  ...,  0.0754,  0.0201,  0.0813],\n",
       "                      [-0.1876,  0.3465,  0.4979,  ..., -0.0436, -0.0362, -0.0866],\n",
       "                      [ 0.1779,  0.3311,  0.3578,  ..., -0.0705,  0.0656,  0.0415],\n",
       "                      ...,\n",
       "                      [-0.0264,  0.2019,  0.1753,  ...,  0.0335,  0.0764,  0.0222],\n",
       "                      [-0.0810, -0.3535, -0.1255,  ..., -0.0645,  0.0299,  0.0438],\n",
       "                      [ 0.0740, -0.1535,  0.1290,  ..., -0.0464, -0.0612,  0.0650]])),\n",
       "             ('input.bias',\n",
       "              tensor([ 0.4091,  0.8057,  0.4696,  0.3282,  0.4459, -0.3094, -0.7575, -0.3531,\n",
       "                      -0.3175,  0.2946,  0.7420,  0.1358,  0.1037, -0.2193, -0.3283])),\n",
       "             ('hidden1.weight',\n",
       "              tensor([[ 0.2575,  0.6953,  0.5631,  0.2704, -0.3716, -0.9438, -0.4709, -0.9932,\n",
       "                       -0.7564, -0.0925, -2.2822,  0.2297,  0.2956,  0.0241, -1.9843],\n",
       "                      [ 0.1413,  0.2127,  0.4845, -0.0701,  0.6703, -0.3185, -0.3957,  0.1315,\n",
       "                       -0.6918, -0.2745, -0.1002,  0.2550,  0.4770,  0.1109, -0.3992],\n",
       "                      [ 0.7823,  0.1207,  0.3213, -0.0475,  0.7223,  0.1428, -0.5531,  0.2000,\n",
       "                        0.0893, -0.3841,  0.1112, -0.0569,  0.5947, -0.6937,  0.4243],\n",
       "                      [ 0.0985,  1.5386, -0.3472, -0.8660,  0.7426, -0.0289, -0.0979, -0.2248,\n",
       "                       -0.0257,  0.4956, -1.0115, -0.2432, -0.4809,  0.6184,  0.6953],\n",
       "                      [ 0.0232, -0.2676, -0.0068,  0.3410, -0.4943,  0.6073,  0.1574,  0.6130,\n",
       "                        0.8132, -0.0182,  1.0047, -0.2493, -0.1170, -0.4713,  0.8951],\n",
       "                      [-0.4531,  0.0077,  0.1982, -0.2570,  0.9576,  1.5319,  1.0911, -0.1294,\n",
       "                       -0.1513,  0.4058,  0.8775, -1.4542, -0.6756,  0.6437, -0.0637],\n",
       "                      [-0.0188,  0.1584, -0.2749,  0.1247,  0.4504,  0.6595,  0.4859, -0.0764,\n",
       "                        0.3448,  0.2107,  0.3694, -0.5843, -0.0219,  0.1169,  0.4314],\n",
       "                      [ 0.2663,  0.4091,  0.4482, -0.0810,  0.6472, -0.0926, -0.1696, -0.2151,\n",
       "                       -0.6046, -0.2196, -0.0106,  0.0498,  0.3613,  0.1652, -0.0080],\n",
       "                      [ 0.0238, -0.7592,  0.3622,  0.2082, -0.1043,  0.3239, -0.0603,  0.2656,\n",
       "                        0.3006, -0.2938,  0.5831, -0.0507, -0.0260, -0.3097,  0.2777],\n",
       "                      [ 0.2607,  0.9150, -0.2169, -0.1303,  0.5222, -0.0239, -0.6132, -0.1086,\n",
       "                       -0.1612,  0.0567,  0.0154,  0.2324, -0.5132,  0.8815,  0.2304],\n",
       "                      [ 0.0487, -0.4816, -0.1768, -0.0849, -0.2191, -0.1497, -0.2482,  0.3167,\n",
       "                        1.2500, -0.0845,  0.8245,  0.4551,  0.0036, -0.5018,  0.0255],\n",
       "                      [-0.0802,  0.5033, -0.0674,  0.0871,  0.2249,  0.4973,  0.3259,  0.4636,\n",
       "                        0.7863, -0.0290,  1.0510, -0.2961, -0.1969,  0.5019,  1.9955],\n",
       "                      [ 0.0207,  0.1539, -0.0628,  0.0138,  0.8202,  0.4971,  0.8012,  0.0564,\n",
       "                       -0.1323, -0.0718, -0.2645, -1.0286, -0.1922,  0.3575,  0.3256],\n",
       "                      [ 0.6240,  0.3164, -0.1598, -0.6158,  0.5465,  0.0340,  0.0693,  0.2907,\n",
       "                        0.7922,  0.1091, -0.0147, -0.6633,  0.0804, -0.1143,  0.5061],\n",
       "                      [ 0.1215,  0.4285,  0.2948, -0.9482,  0.0128, -0.8172, -0.5179, -0.7863,\n",
       "                       -0.8570,  0.0745, -0.5490,  0.6721,  0.2755, -0.2263, -0.3991],\n",
       "                      [-1.0201,  0.0455, -0.9147, -0.7720, -0.4775,  0.5190,  0.5954,  0.3022,\n",
       "                        0.0520,  0.4095,  0.3205, -0.5907, -0.5462, -0.1718,  1.2266],\n",
       "                      [-0.1344, -0.1653, -0.2833, -0.3142, -0.1152,  0.2869,  0.2829,  0.0184,\n",
       "                        0.1363,  0.3274, -0.0947, -0.3415, -0.1789,  0.0378,  0.0306],\n",
       "                      [-0.1025,  0.6285, -0.0914, -0.1482,  0.0396, -0.4274,  0.1675, -0.2574,\n",
       "                       -0.0724, -0.0152,  0.2160, -0.1630,  0.1252,  0.1318,  0.1642],\n",
       "                      [ 0.5360,  0.7464,  0.3229, -0.2490,  0.6192, -0.1805, -0.1130,  0.2933,\n",
       "                       -0.0098,  0.0768,  1.5201, -0.4075,  0.0207,  0.5212,  1.6912],\n",
       "                      [ 0.5786, -0.6360,  0.2668,  0.7761, -0.6159, -1.0443, -0.5363, -0.5601,\n",
       "                       -0.9773,  0.1997, -2.5120,  0.8936,  0.7410,  0.4194, -1.9675]])),\n",
       "             ('hidden1.bias',\n",
       "              tensor([ 0.6749,  0.1967,  0.3827, -0.8974, -0.0010, -0.6915,  0.1325,  0.2587,\n",
       "                      -0.1166, -0.4284,  0.2775,  0.1698, -0.1733, -0.5159, -0.1306, -0.6334,\n",
       "                      -0.0710, -0.0774,  0.4660,  0.7498])),\n",
       "             ('hidden2.weight',\n",
       "              tensor([[-1.1929,  0.7590, -0.4203,  0.6335, -0.5118, -0.6470,  0.1136,  0.7656,\n",
       "                       -0.2115,  0.4258, -0.1070,  1.3044, -0.3927,  0.4815, -0.2065,  0.1193,\n",
       "                        0.5277, -0.0628,  1.0586, -1.2823],\n",
       "                      [-0.1084, -0.3936,  0.2931, -0.8568,  0.0422,  0.2233, -0.1856, -0.4776,\n",
       "                        0.3987, -0.6149,  0.4459, -0.1198,  0.2158, -0.1368, -0.3301, -0.5805,\n",
       "                        0.1345, -0.3114, -0.2366,  0.5852],\n",
       "                      [ 1.2269, -0.5441, -0.0556,  0.6234,  0.3867,  0.3451,  0.2686, -0.6395,\n",
       "                       -0.5229,  0.2870, -0.3788, -1.0749,  0.3720, -0.4522,  0.4057,  0.1022,\n",
       "                       -0.1416, -0.1017, -0.5362,  0.9197]])),\n",
       "             ('hidden2.bias', tensor([ 0.6436,  0.2206, -0.8653]))])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lang_classifier)\n",
    "lang_classifier.state_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Convert the following dev data into the input format for the neural network above. \n",
    "\n",
    "**Hint** The indices of the characters are based on the order in the training data, and should match in the development data, we provide the correct idx2char and char2idx that were used to train the model in the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "wooki_dev_text, wooki_dev_labels = load_langid('langid-data/wookipedia_langid.dev.tok.txt')\n",
    "\n",
    "BOL_dev_matrix=torch.zeros(len(wooki_dev_text), len(idx2char), dtype=torch.float)\n",
    "\n",
    "for s, sentence in enumerate(wooki_dev_text):\n",
    "    for i in range(len(idx2char)):\n",
    "        if idx2char[i] in sentence:\n",
    "            BOL_dev_matrix[s][i]=1\n",
    "#BOL_list[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) run a forward pass on the dev-data with `lang_classifier`, using the forward() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3604,  0.7146,  0.3900],\n",
       "        [-2.2557,  1.4831, -0.5462],\n",
       "        [-2.0805,  1.3734, -0.5807],\n",
       "        ...,\n",
       "        [ 2.0050, -1.2380, -1.6113],\n",
       "        [ 2.2639, -1.3041, -1.8159],\n",
       "        [-1.7680, -1.5234,  2.6722]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Output_1=lang_classifier.forward(BOL_dev_matrix)\n",
    "Output_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* c) Apply your manual implementation of the forward pass to the evaluation data by using the parameters (weights) you just loaded with `state_dict()`. This allows you to check if you get the same results back as the model implemented in Torch. If the outputs match, you implemented the forward pass correctly, congratulations!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output z3 (before softmax) for all samples in the training set: tensor([[-0.9823,  0.6135,  0.3713],\n",
      "        [-0.9783,  0.9021, -0.4977],\n",
      "        [-0.9693,  0.8795, -0.5232],\n",
      "        ...,\n",
      "        [ 0.9644, -0.8449, -0.9233],\n",
      "        [ 0.9786, -0.8628, -0.9484],\n",
      "        [-0.9434, -0.9093,  0.9905]])\n"
     ]
    }
   ],
   "source": [
    "# Extracting the layers from the state_dict\n",
    "weights1 = lang_classifier.state_dict()['input.weight'].T\n",
    "bias1 = lang_classifier.state_dict()['input.bias'].T\n",
    "weights2 = lang_classifier.state_dict()['hidden1.weight'].T\n",
    "bias2 = lang_classifier.state_dict()['hidden1.bias'].T\n",
    "weights3 = lang_classifier.state_dict()['hidden2.weight'].T\n",
    "bias3 = lang_classifier.state_dict()['hidden2.bias'].T\n",
    "\n",
    "# Now you can use these globally across your script\n",
    "\n",
    "\n",
    "# Perform the forward pass on the entire training set\n",
    "Output_2 = forward_pass(BOL_dev_matrix)\n",
    "\n",
    "print(\"Output z3 (before softmax) for all samples in the training set:\", Output_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: internally the torch model saves the weight in a transposed vector for efficiency reasons. This means that W1 will have the dimension of (15,131). To use your previous implementation you have to call the the transpose function in Pytorch ([`.t()`](https://pytorch.org/docs/stable/generated/torch.t.html)), which will convert the shape to be (131,15)\n",
    "\n",
    "* d) Now apply softmax on the resulting weights and convert the output to the label predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax 1 \n",
      " tensor([[0.0261, 0.5653, 0.4086],\n",
      "        [0.0206, 0.8657, 0.1138],\n",
      "        [0.0270, 0.8523, 0.1208],\n",
      "        ...,\n",
      "        [0.9381, 0.0366, 0.0252],\n",
      "        [0.9568, 0.0270, 0.0162],\n",
      "        [0.0115, 0.0147, 0.9738]], grad_fn=<SoftmaxBackward0>)\n",
      "softmax 2 \n",
      " tensor([[0.1020, 0.5031, 0.3949],\n",
      "        [0.1090, 0.7147, 0.1763],\n",
      "        [0.1122, 0.7126, 0.1753],\n",
      "        ...,\n",
      "        [0.7603, 0.1245, 0.1151],\n",
      "        [0.7668, 0.1216, 0.1116],\n",
      "        [0.1117, 0.1156, 0.7727]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "softmax_result1 = F.softmax(Output_1, dim=1)\n",
    "print(\"softmax 1 \\n\", softmax_result1)\n",
    "\n",
    "softmax_result2 = F.softmax(Output_2, dim=1)\n",
    "print(\"softmax 2 \\n\" ,softmax_result2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### not quite the same but pretty close, not sure why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: Word2vec and PyTorch\n",
    "In the following exercises, you are going to explore what is represented with word embeddings. You are going to make use of the python gensim package and two sets of pre-trained embeddings. The embeddings can be downloaded from:\n",
    "\n",
    "* http://itu.dk/people/robv/data/embeds/twitter.bin.tar.gz\n",
    "* http://itu.dk/people/robv/data/embeds/GoogleNews-50k.bin.tar.gz\n",
    "\n",
    "The first embeddings are skip-gram embeddings trained on a collection of 2 billion words from English tweets collected during 2012 and 2018 with the default settings of word2vec. The second embeddings are trained on 100 billion words from Google News. They have both been truncated to the most frequent 500,000 words. Note that loading that each of these embeddings require approximately 2GB of ram.\n",
    "\n",
    "The embeddings can be loaded in gensim as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data=r\"C:\\Users\\elias\\Desktop\\_ITU\\4.semester\\NLP\\twitter.bin\"\n",
    "google_data=r\"C:\\Users\\elias\\Desktop\\_ITU\\'4.semester\\NLP\\GoogleNews-50k.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading finished\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "\n",
    "twitEmbs = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "                                twitter_data, binary=True)\n",
    "print('loading finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use the index operator ``[]`` or the function ``get_vector()`` to acces the individual word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitEmbs['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word similarities\n",
    "Cosine distance can be used to measure the distance between two words. It is defined as:\n",
    "\\begin{equation}\n",
    "cos_{\\vec{a},\\vec{b}} = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}| |\\vec{b}|} = \\frac{\\sum^n_1 a_i b_i}{\\sqrt{\\sum^n_1 a_i^2} \\sqrt{\\sum^n_1 b_i^2}}\n",
    "\\end{equation}\n",
    "\n",
    "* a) Implement the cosine similarity using pure python (using only the ``math`` package, no other libraries). Note that `similarity == 1-distance`.\n",
    "\n",
    "You can compare your scores to the gensim implementation to check wheter it is correct. The following code should give the same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10446518659591675\n",
      "0.10446513714193606\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def cosine(vec1, vec2):\n",
    "    dot= sum(vec1*vec2)\n",
    "    len1=math.sqrt(sum(vec1*vec1))\n",
    "    len2=math.sqrt(sum(vec2*vec2))\n",
    "    return 1-dot/(len1*len2)\n",
    "\n",
    "print(twitEmbs.distance('cat', 'dog'))\n",
    "print(cosine(twitEmbs['cat'], twitEmbs['dog']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In wordnet, the distance between two senses can be based on the distance in the taxonomy. The most common metric for this is:\n",
    "\n",
    "* Wu-Palmer Similarity: denotes how similar two word senses are, based on the depth of the two senses in the taxonomy and of their Least Common Subsumer (most specific ancestor node).\n",
    "\n",
    "It can be obtained in python like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet similarity: 0.8571428571428571\n",
      "Twitter similarity: 0.8955348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "first_word = wordnet.synsets('cat')[0] #0 means: most common sense\n",
    "second_word = wordnet.synsets('dog')[0]\n",
    "print('WordNet similarity: ' + str(first_word.wup_similarity(second_word)))\n",
    "\n",
    "print('Twitter similarity: ' + str(twitEmbs.similarity('cat', 'dog')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* b) Think of 5 word pairs which have a high similarity according to you. Estimate the difference between these pairs in wordnet as well as in the Twitter embeddings and the Google News embeddings. Which method is closest to your own intuition? (You can use the gensim implementation of cosine similarity here.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knight  and  warrior  : \n",
      "WordNet similarity: 0.631578947368421\n",
      "Twitter similarity: 0.59290004\n",
      "steak  and  beef  : \n",
      "WordNet similarity: 0.15384615384615385\n",
      "Twitter similarity: 0.68714267\n",
      "bench  and  squat  : \n",
      "WordNet similarity: 0.09090909090909091\n",
      "Twitter similarity: 0.5594138\n",
      "dick  and  cock  : \n",
      "WordNet similarity: 0.21052631578947367\n",
      "Twitter similarity: 0.8291812\n",
      "doctor  and  nurse  : \n",
      "WordNet similarity: 0.8695652173913043\n",
      "Twitter similarity: 0.8612271\n",
      "bike  and  bicycle  : \n",
      "WordNet similarity: 0.7272727272727273\n",
      "Twitter similarity: 0.8499112\n",
      "cheese  and  milk  : \n",
      "WordNet similarity: 0.875\n",
      "Twitter similarity: 0.7525767\n"
     ]
    }
   ],
   "source": [
    "fword = [\"knight\",\"steak\",\"bench\",\"dick\",\"doctor\",\"bike\", \"cheese\"]\n",
    "sword = [\"warrior\",\"beef\",\"squat\",\"cock\",\"nurse\",\"bicycle\", \"milk\"]\n",
    "\n",
    "\n",
    "for i in range(7):\n",
    "    first_word = wordnet.synsets(fword[i])[0] \n",
    "    second_word = wordnet.synsets(sword[i])[0]\n",
    "\n",
    "    print(fword[i],\" and \",sword[i],\" : \")\n",
    "\n",
    "    print('WordNet similarity: ' + str(first_word.wup_similarity(second_word)))\n",
    "    print('Twitter similarity: ' + str(twitEmbs.similarity(fword[i], sword[i])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a Language Identification model using PyTorch\n",
    "\n",
    "#### **Objective**  \n",
    "Implement a PyTorch-based model for **language identification**. The goal is to train a model that can classify input text into three different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) **Prepare Training Data.** \n",
    "\n",
    "Prepare the training data (texts) in an n-hot format, and the labels in a seperate matrix. The training data will be of shape (15000, 131) for 15000 instances and 131 features (i.e. characters). The train labels will be of shape (15000), and contains labels representing the languages (e.g. 0,1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15000, 131])\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "wooki_train_text, wooki_train_labels = load_langid('langid-data/wookipedia_langid.train.tok.txt')\n",
    "\n",
    "BOL_train_matrix=torch.zeros(len(wooki_train_text), len(idx2char), dtype=torch.float)\n",
    "\n",
    "for s, sentence in enumerate(wooki_train_text):\n",
    "    for i in range(len(idx2char)):\n",
    "        if idx2char[i] in sentence:\n",
    "            BOL_train_matrix[s][i]=1\n",
    "\n",
    "\n",
    "print(BOL_train_matrix.shape)  # \n",
    "print(len(wooki_train_labels))  # \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =BOL_train_matrix\n",
    "train_labels = torch.tensor([label2idx[label] for label in wooki_train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15000])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) **Initialize Model, Loss Function, and Optimizer**\n",
    "\n",
    "You can use the model as defined in assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(BOL_train_matrix, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* c) **Prepare dataset and dataloader**\n",
    "\n",
    "You can use torch.utils.data.DataLoader and torch.utils.data.TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize langClassifier using LangId with vocab size as an argument\n",
    "vocab_size = len(idx2char)\n",
    "langClassifier = LangId(vocab_size)\n",
    "\n",
    "# CrossEntropyLoss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD optimizer with learning rate 0.00001 and momentum 0.9\n",
    "optimizer = optim.SGD(langClassifier.parameters(), lr=0.00001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* d) **Implement the training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.073342069125633\n",
      "Epoch [2/20], Loss: 1.0723590652571557\n",
      "Epoch [3/20], Loss: 1.0713857114950478\n",
      "Epoch [4/20], Loss: 1.0703879546509114\n",
      "Epoch [5/20], Loss: 1.0693813336175133\n",
      "Epoch [6/20], Loss: 1.0683769140162194\n",
      "Epoch [7/20], Loss: 1.067360745818376\n",
      "Epoch [8/20], Loss: 1.0663304509384546\n",
      "Epoch [9/20], Loss: 1.0652965464825823\n",
      "Epoch [10/20], Loss: 1.0642437657821915\n",
      "Epoch [11/20], Loss: 1.0631836299448887\n",
      "Epoch [12/20], Loss: 1.0621023999094201\n",
      "Epoch [13/20], Loss: 1.0610235527888545\n",
      "Epoch [14/20], Loss: 1.0599193822092086\n",
      "Epoch [15/20], Loss: 1.0587940765088046\n",
      "Epoch [16/20], Loss: 1.057648436601228\n",
      "Epoch [17/20], Loss: 1.0564977339844206\n",
      "Epoch [18/20], Loss: 1.0553157456648121\n",
      "Epoch [19/20], Loss: 1.0541284602842351\n",
      "Epoch [20/20], Loss: 1.0528840591658408\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of epochs\n",
    "num_epochs = 20\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Run the forward pass on langClassifier model with batch_data and collect the outputs\n",
    "            outputs = model(batch_data)\n",
    "\n",
    "            # Calculate the loss of the outputs with respect to batch_labels\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "\n",
    "            # Perform backpropagation of the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model weights using the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "            \n",
    "train(langClassifier, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "\n",
    "    # Print the loss at the end of each epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* e) **Save the Model**\n",
    "\n",
    "Save the trained langClassifier model using torch.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as langClassifier_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(langClassifier.state_dict(), 'langClassifier_model.pth')\n",
    "print(\"Model saved as langClassifier_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* f) **Load and Prepare Development Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "wooki_dev_text, wooki_dev_labels = load_langid('langid-data/wookipedia_langid.dev.tok.txt')\n",
    "\n",
    "# Convert development data to n-hot format\n",
    "BOL_dev_matrix = torch.zeros(len(wooki_dev_text), len(idx2char), dtype=torch.float)\n",
    "for s, sentence in enumerate(wooki_dev_text):\n",
    "    for i in range(len(idx2char)):\n",
    "        if idx2char[i] in sentence:\n",
    "            BOL_dev_matrix[s][i] = 1\n",
    "\n",
    "# Convert labels to numeric format\n",
    "dev_labels_numeric = torch.tensor([label2idx[label] for label in wooki_dev_labels])\n",
    "\n",
    "dev_dataset = TensorDataset(BOL_dev_matrix, dev_labels_numeric)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* g) **Evaluate the Model**\n",
    "Evaluate the results on dev data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elias\\AppData\\Local\\Temp\\ipykernel_8132\\1553865055.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  langClassifier.load_state_dict(torch.load('langClassifier_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "langClassifier.load_state_dict(torch.load('langClassifier_model.pth'))\n",
    "langClassifier.eval()\n",
    "\n",
    "# Forward pass on dev_data with langClassifier model\n",
    "logits = langClassifier(BOL_dev_matrix)\n",
    "\n",
    "# Obtain the prediction with the highest logit\n",
    "predicted = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Count the correct labels\n",
    "correct = (predicted == dev_labels_numeric).sum().item()\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = correct / len(dev_labels_numeric)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* h) Explore the tuning of one part of the model to improve performance, you can choose for example to tune the learning rate, change the number of layers, dimensions of layers, or prune the input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.0304788420957798\n",
      "Epoch [2/20], Loss: 0.7167769280959294\n",
      "Epoch [3/20], Loss: 0.47726635053467903\n",
      "Epoch [4/20], Loss: 0.4150668075407492\n",
      "Epoch [5/20], Loss: 0.39715405755332794\n",
      "Epoch [6/20], Loss: 0.38949856432134916\n",
      "Epoch [7/20], Loss: 0.38486340586374057\n",
      "Epoch [8/20], Loss: 0.3826969844032961\n",
      "Epoch [9/20], Loss: 0.380083731306133\n",
      "Epoch [10/20], Loss: 0.37808633986503076\n",
      "Epoch [11/20], Loss: 0.37775072843027013\n",
      "Epoch [12/20], Loss: 0.3761742014302882\n",
      "Epoch [13/20], Loss: 0.3744962166494398\n",
      "Epoch [14/20], Loss: 0.3738096256309481\n",
      "Epoch [15/20], Loss: 0.37315579996243725\n",
      "Epoch [16/20], Loss: 0.37216841526377176\n",
      "Epoch [17/20], Loss: 0.37206720112801106\n",
      "Epoch [18/20], Loss: 0.37124445643633414\n",
      "Epoch [19/20], Loss: 0.3697653824904326\n",
      "Epoch [20/20], Loss: 0.36974122748573196\n",
      "Accuracy: 85.90%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_and_evaluate(model, train_loader, num_epochs=20, learning_rate=0.00001, momentum=0.9):\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # Training loop\n",
    "    train(langClassifier, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "    # Evaluation on development set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(BOL_dev_matrix)\n",
    "        predicted = torch.argmax(logits, dim=1)\n",
    "        correct = (predicted == dev_labels_numeric).sum().item()\n",
    "        accuracy = correct / len(dev_labels_numeric)\n",
    "        print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Example usage:\n",
    "# Initialize the model with new parameters if needed\n",
    "vocab_size = len(idx2char)\n",
    "langClassifier = LangId(vocab_size)\n",
    "\n",
    "# Retrain and evaluate the model with new parameters\n",
    "train_and_evaluate(langClassifier, train_loader, num_epochs=20, learning_rate=0.001, momentum=0.9)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
