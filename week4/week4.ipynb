{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - NLP and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "# Lecture 7. RNN\n",
    "\n",
    "In assignments for this lecture we are going to implement an RNN POS tagger in Pytorch.\n",
    "\n",
    "You can use the following function for data reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12543\n",
      "2000\n",
      "159\n"
     ]
    }
   ],
   "source": [
    "def read_conll_file(path):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[0])\n",
    "            current_tags.append(tok[1])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data\n",
    "\n",
    "train_data = read_conll_file('pos-data/en_ewt-train.conll')\n",
    "dev_data = read_conll_file('pos-data/en_ewt-dev.conll')\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(dev_data))\n",
    "print(max([len(x[0]) for x in train_data ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data for use in PyTorch\n",
    "\n",
    "* a) Convert the data to a format that can be used in a Pytorch module. This means we require:\n",
    "\n",
    "  * training data: matrix of number of instances (12543) by the maximum sentence length (159), filled with word indices\n",
    "  * training labels: matrix of the same size in the first dimension, but filled with label indexes instead ( total of 17)\n",
    "  * the same two sets for the dev data (note that no word indices can be added anymore)\n",
    "  \n",
    "A special `<PAD>` token can be used for padding, for sentences shorter as 159 words. For the unknown words in the test set, you can use the `<PAD>` token as well.\n",
    "\n",
    "**HINT** It will be beneficial in the long run to make a function to convert your data to the right format, as you would have to do it for the train, dev and test sets, and for any other dataset you want to evaluate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, pad_unk='<PAD>'):\n",
    "        \"\"\"\n",
    "        A convenience class that can help store a vocabulary\n",
    "        and retrieve indices for inputs.\n",
    "        \"\"\"\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {self.pad_unk: 0}\n",
    "        self.idx2word = [self.pad_unk]\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "            else:\n",
    "                return self.word2idx[self.pad_unk]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word(idx)\n",
    "    \n",
    "\n",
    "max_len= max([len(x[0]) for x in train_data ])\n",
    "\n",
    "# Your implementation goes here:\n",
    "wordvocab = Vocab()\n",
    "for sentence, _ in train_data:\n",
    "    for word in sentence:\n",
    "        wordvocab.getIdx(word, add=True)  # Add words to vocab\n",
    "\n",
    "classvocab = Vocab()\n",
    "for _, class_sentence in train_data:\n",
    "    for word in class_sentence:\n",
    "        classvocab.getIdx(word, add=True)  # Add words to vocab\n",
    "\n",
    "\n",
    "def convert_data(data):  \n",
    "    sentece_word_matrix=torch.zeros(len(train_data), max_len, dtype=torch.long)\n",
    "    for s, sentece in  enumerate(data):\n",
    "        for w, word in enumerate(sentece[0]):\n",
    "            sentece_word_matrix[s][w]=wordvocab.getIdx(word)\n",
    "    return sentece_word_matrix\n",
    "\n",
    "def convert_label(data):  \n",
    "    sentece_word_matrix=torch.zeros(len(train_data), max_len, dtype=torch.long)\n",
    "    for s, sentece in  enumerate(data):\n",
    "        for w, word in enumerate(sentece[1]):\n",
    "            sentece_word_matrix[s][w]=classvocab.getIdx(word)\n",
    "    return sentece_word_matrix\n",
    "\n",
    "idx2word=convert_data(train_data)\n",
    "idx2label=convert_label(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) Until now, we have used a batch-size of 1 in our implemented models, meaning that the models weights are updated after each sentence. This is not very computationally efficient. Larger batch-sizes increase the training speed, and can also lead to better performance (more stable training). You can easily convert existing training data to batches, by splitting it up in chunks of `batch_size` sentences, like this (*Make sure you understand this code!*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # 200 instances, 100 features/weights\n",
    "# tmp_feats = torch.zeros((200, 100))\n",
    "\n",
    "# batch_size = 32\n",
    "# num_batches = int(len(tmp_feats)/batch_size)\n",
    "\n",
    "# print(num_batches)\n",
    "\n",
    "# print(tmp_feats.shape)\n",
    "\n",
    "# tmp_feats_batches = tmp_feats[:batch_size*num_batches].view(num_batches,batch_size, 100)\n",
    "\n",
    "# # 6 batches with 32 instances with 100 features\n",
    "# print(tmp_feats_batches.shape)\n",
    "\n",
    "# print()\n",
    "# for feats_batch in tmp_feats_batches:\n",
    "#     print(feats_batch.shape)\n",
    "#     # Here you can call forward/calculate the loss etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this throws away a tiny part of the data (the last `len(tmp_feats)%batch_size`=6 samples), an alternative would be to pad, and ignore the padded part of the last batch for the loss. For the following assignments you can leave the remaining samples out (note that the dev set is dividable by 16 in this case). Furthermore, note that PyTorch supports a more advanced method for batching: [data loaders](https://pytorch.org/docs/stable/data.html), which we will not cover in this course (but you can use them for the final project).\n",
    "\n",
    "Convert your training data and labels to batches of batch size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783\n",
      "torch.Size([783, 16, 159])\n",
      "torch.Size([783, 16, 159])\n"
     ]
    }
   ],
   "source": [
    "# Your implementation goes here:\n",
    "# 200 instances, 100 features/weights\n",
    "tmp_feats = idx2word\n",
    "tmp_label = idx2label\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "num_batches = int(len(tmp_feats)/batch_size)\n",
    "\n",
    "print(num_batches)\n",
    "\n",
    "#print(tmp_feats.shape)\n",
    "\n",
    "tmp_feats_batches = tmp_feats[:batch_size*num_batches].view(num_batches,batch_size, 159)\n",
    "tmp_label_batches = tmp_label[:batch_size*num_batches].view(num_batches,batch_size, 159)\n",
    "\n",
    "\n",
    "# 6 batches with 32 instances with 100 features\n",
    "print(tmp_feats_batches.shape)\n",
    "print(tmp_label_batches.shape)\n",
    "#print()\n",
    "#for feats_batch in tmp_feats_batches:\n",
    " #   print(feats_batch.shape)\n",
    "    # Here you can call forward/calculate the loss etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_label_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Train an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Implement a POS tagger model in Pytorch using a [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer for word representations and a [`torch.nn.RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) layer. You can use a [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer for prediction of the label. Train this tagger on the language identification data, and evaluate its performance. Note that during each training step, you now get the predictions and loss on a whole batch directly. Use the following hyperparameters: 5 epochs over the full training data, word embeddings dimension: 100, rnn dimension of 50, learning rate of 0.01 in an [Adam optimizer](https://pytorch.org/docs/stable/optim.html) and a [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "\n",
    "Hints:\n",
    "* **Set batch_first to true!**, as explained on the [`torch.nn.RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) page. By default the RNN expects the input to be in the shape: `(seq_len, batch, rnn_dim)`, when it is set to true it should be: `(batch, seq_len, rnn_dim)`, which is usually easier to work with.\n",
    "* Training an RNN is generally much slower compared to the machine learning models we implemented before on this data, so we suggest to start with only a sub-part of the data, like 100 or 1,000 sentences. It is also ok to use only 1,000 sentences for your final model (or use the HPC to train the full model).\n",
    "* To calculate the cross entropy loss, we need the predictions to be in the first dimension. We can convert the predictions values from our model (32\\*159\\*18 for 1 batch) to a flatter representation (5088\\*18) by using: `.view(BATCH_SIZE * max_len, -1)`. Of course, we also have to adapt the labels from 32\\*159 to 5088\\*1.\n",
    "\n",
    "For more information on how to implement a Pytorch module, we refer to the code used to obtain the weights in the assignment of week 3 (`week4/train_ff.py`), and the following tutorial series: https://pytorch.org/tutorials/beginner/nlp/index.html (especially the 2nd and 4th tutorials are relevant). You can use the code below as a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 batch: 0 - 50 avg loss:  918.8829306793212\n",
      "epoch:  0 batch: 50 - 100 avg loss:  610.730962524414\n",
      "epoch:  0 batch: 100 - 150 avg loss:  304.3568670654297\n",
      "epoch:  0 batch: 150 - 200 avg loss:  252.72033966064453\n",
      "epoch:  0 batch: 200 - 250 avg loss:  261.7361990356445\n",
      "epoch:  0 batch: 250 - 300 avg loss:  183.8376300048828\n",
      "epoch:  0 batch: 300 - 350 avg loss:  212.5595167541504\n",
      "epoch:  0 batch: 350 - 400 avg loss:  256.87939453125\n",
      "epoch:  0 batch: 400 - 450 avg loss:  271.08595947265627\n",
      "epoch:  0 batch: 450 - 500 avg loss:  182.6119920349121\n",
      "epoch:  0 batch: 500 - 550 avg loss:  185.7048812866211\n",
      "epoch:  0 batch: 550 - 600 avg loss:  151.22944442749022\n",
      "epoch:  0 batch: 600 - 650 avg loss:  137.38857650756836\n",
      "epoch:  0 batch: 650 - 700 avg loss:  142.20396263122558\n",
      "epoch:  0 batch: 700 - 750 avg loss:  147.8579244995117\n",
      "epoch:  1 batch: 0 - 50 avg loss:  146.02106033325197\n",
      "epoch:  1 batch: 50 - 100 avg loss:  105.74485786437988\n",
      "epoch:  1 batch: 100 - 150 avg loss:  70.8334461593628\n",
      "epoch:  1 batch: 150 - 200 avg loss:  75.3751141357422\n",
      "epoch:  1 batch: 200 - 250 avg loss:  91.14384841918945\n",
      "epoch:  1 batch: 250 - 300 avg loss:  72.73080577850342\n",
      "epoch:  1 batch: 300 - 350 avg loss:  83.3033812713623\n",
      "epoch:  1 batch: 350 - 400 avg loss:  104.87809173583985\n",
      "epoch:  1 batch: 400 - 450 avg loss:  121.76111740112304\n",
      "epoch:  1 batch: 450 - 500 avg loss:  101.68155372619628\n",
      "epoch:  1 batch: 500 - 550 avg loss:  109.47957107543945\n",
      "epoch:  1 batch: 550 - 600 avg loss:  105.81842521667481\n",
      "epoch:  1 batch: 600 - 650 avg loss:  104.00889282226562\n",
      "epoch:  1 batch: 650 - 700 avg loss:  108.21954216003418\n",
      "epoch:  1 batch: 700 - 750 avg loss:  125.35157432556153\n",
      "epoch:  2 batch: 0 - 50 avg loss:  97.14756462097168\n",
      "epoch:  2 batch: 50 - 100 avg loss:  74.24294235229492\n",
      "epoch:  2 batch: 100 - 150 avg loss:  51.12787872314453\n",
      "epoch:  2 batch: 150 - 200 avg loss:  54.13667713165283\n",
      "epoch:  2 batch: 200 - 250 avg loss:  68.76094369888305\n",
      "epoch:  2 batch: 250 - 300 avg loss:  50.139966373443606\n",
      "epoch:  2 batch: 300 - 350 avg loss:  61.96844854354858\n",
      "epoch:  2 batch: 350 - 400 avg loss:  83.41031005859375\n",
      "epoch:  2 batch: 400 - 450 avg loss:  91.3241219329834\n",
      "epoch:  2 batch: 450 - 500 avg loss:  87.38061492919923\n",
      "epoch:  2 batch: 500 - 550 avg loss:  99.61700057983398\n",
      "epoch:  2 batch: 550 - 600 avg loss:  103.37988822937012\n",
      "epoch:  2 batch: 600 - 650 avg loss:  87.7562752532959\n",
      "epoch:  2 batch: 650 - 700 avg loss:  94.41176132202149\n",
      "epoch:  2 batch: 700 - 750 avg loss:  118.32592330932617\n",
      "epoch:  3 batch: 0 - 50 avg loss:  88.14637908935546\n",
      "epoch:  3 batch: 50 - 100 avg loss:  70.06395179748534\n",
      "epoch:  3 batch: 100 - 150 avg loss:  41.54449073791504\n",
      "epoch:  3 batch: 150 - 200 avg loss:  53.68122917175293\n",
      "epoch:  3 batch: 200 - 250 avg loss:  78.07749214172364\n",
      "epoch:  3 batch: 250 - 300 avg loss:  48.37950506210327\n",
      "epoch:  3 batch: 300 - 350 avg loss:  49.532002983093264\n",
      "epoch:  3 batch: 350 - 400 avg loss:  65.68192470550537\n",
      "epoch:  3 batch: 400 - 450 avg loss:  85.65785083770751\n",
      "epoch:  3 batch: 450 - 500 avg loss:  78.07024208068847\n",
      "epoch:  3 batch: 500 - 550 avg loss:  93.14502208709717\n",
      "epoch:  3 batch: 550 - 600 avg loss:  86.23389339447021\n",
      "epoch:  3 batch: 600 - 650 avg loss:  75.62763763427735\n",
      "epoch:  3 batch: 650 - 700 avg loss:  84.38965545654297\n",
      "epoch:  3 batch: 700 - 750 avg loss:  112.68378005981445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaggerModel(\n",
       "  (embedding): Embedding(19671, 159)\n",
       "  (rnn): RNN(159, 50, batch_first=True)\n",
       "  (linear): Linear(in_features=50, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "DIM_EMBEDDING = 159\n",
    "RNN_HIDDEN = 50\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 4\n",
    "\n",
    "class TaggerModel(torch.nn.Module):\n",
    "    def __init__(self, nwords, ntags):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(nwords,DIM_EMBEDDING)\n",
    "        self.rnn = nn.RNN(DIM_EMBEDDING, RNN_HIDDEN, batch_first=True)  # Set batch_first=True\n",
    "        self.linear = nn.Linear(RNN_HIDDEN,ntags)\n",
    "        # TODO\n",
    "        \n",
    "    def forward(self, inputData):\n",
    "        word_vectors =  self.embedding(inputData)# TODO\n",
    "        rnn_out, _ = self.rnn(word_vectors)        \n",
    "        tag_scores = self.linear(rnn_out)        \n",
    "        return tag_scores\n",
    "        \n",
    "#model = TaggerModel(len(idx2word),len(tmp_label))\n",
    "model = TaggerModel(len(wordvocab.idx2word), len(classvocab.idx2word))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()   \n",
    "    # reset the gradient\n",
    "    model.zero_grad()\n",
    "    loss_last_50 = []\n",
    "    # loop over batches\n",
    "    for batch_num, (batch, label) in enumerate(zip(tmp_feats_batches, tmp_label_batches), start=1):\n",
    "        #print(batch.max().item(), len(idx2word))\n",
    "        predicted_values = model.forward(batch)\n",
    "        loss= loss_function(predicted_values.view(batch_size*max_len, -1), label.view(-1))\n",
    "        # update\n",
    "        loss.backward()\n",
    "        loss_last_50.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_num % 50 == 0:\n",
    "            print(\"epoch: \", epoch, \"batch:\",batch_num-50,\"-\",batch_num, \"avg loss: \", sum(loss_last_50) / 50)\n",
    "            loss_last_50 = []\n",
    "        \n",
    "\n",
    "\n",
    "# set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) Now evaluate the tagger on the dev data (`pos-data/en_ewt-dev.conll`) with accuracy (make sure to not count the padding tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dev_word_matrix = convert_data(dev_data)\n",
    "dev_label_matrix = convert_label(dev_data)\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "all_predicted = []\n",
    "all_true = []\n",
    "# Predict the entire batch at once\n",
    "with torch.no_grad():\n",
    "    tag_scores = model(dev_word_matrix)\n",
    "\n",
    "# Get predicted tag indices and flatten\n",
    "_, predicted_tags = torch.max(tag_scores, dim=-1)\n",
    "all_predicted = predicted_tags.view(-1).cpu().numpy()\n",
    "all_true = dev_label_matrix.view(-1).cpu().numpy()\n",
    "\n",
    "# Mask out padding (index 0)\n",
    "mask = all_true != 0\n",
    "all_predicted = all_predicted[mask]\n",
    "all_true = all_true[mask]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_true, all_predicted)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement a Bi-RNN in Pytorch\n",
    "In this assignment we are going to implement a bidirectional RNN classifier in Pytorch including dropout layers, and train it for the task of topic classification.\n",
    "\n",
    "You can use the following function for data reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_topics(path):\n",
    "    text = []\n",
    "    labels = []\n",
    "    for lineIdx, line in enumerate(open(path)):\n",
    "        tok = line.strip().split('\\t')\n",
    "        labels.append(tok[0])\n",
    "        text.append(tok[1].split(' '))\n",
    "    return text, labels\n",
    "\n",
    "topic_train_text, topic_train_labels = load_topics('topic-data/train.txt')\n",
    "topic_dev_text, topic_dev_labels = load_topics('topic-data/dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Convert the data to a format that can be used in a Pytorch module. In this assignment, we can cap the size of an utterance, as each utterance only needs 1 label. Use a maximum length of 64 words, for longer sentences, only keep the first 64 words. A special `<PAD>` token can be used for padding for sentences shorter as 64 words. For the unknown words in the test set, you can use the `<PAD>` token as well.\n",
    "\n",
    "**hint**: the shape of the training data should be 13,000 by 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Your implementation goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) Convert your input into batches of size 32, similar as you did in assignment 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* c) Implement a classification model in Pytorch using a [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer and a [`torch.nn.RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) layer. Train this classification model on the language identification data, and evaluate its performance. Note that during each training step, you now get the predictions and loss on a whole batch directly. Use the following hyperparameters: 5 epochs over the full training data, word embeddings dimension: 100, rnn dimension of 50, learning rate of 0.01 in an [Adam optimizer](https://pytorch.org/docs/stable/optim.html) and a [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "\n",
    "Hints:\n",
    "* see also the hints for assignment2a\n",
    "* Set bidirectional=True for the RNN layer (so that we are training a RNN), note that the input dimensions of the next layer should then be rnn_dim*2. \n",
    "* We use words as inputs, and need only one label per sentence, so you should use the output of the last item from the forward layer, and the output of the first item for the backward layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: In torch, the BiRNN returns a concatenation of the forward and \n",
    "# backward layer. Here is an example of how these can be extracted again\n",
    "#     backward_out = rnn_out[:,0,-size:].squeeze()\n",
    "#     forward_out = rnn_out[:,-1,:size].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* d) Add a `torch.nn.Dropout` layer with a masking probability of 0.2 between the word embeddings and the RNN layer and\n",
    "  another dropout layer with a masking probability of 0.3 between the rnn layer and the output layer. Evaluate the\n",
    "  performance again, is the performance higher?, why would this be the case?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
